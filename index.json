[{"authors":null,"categories":null,"content":"I\u0026rsquo;m a fourth-year PhD student in the Interactive Geometry Lab at ETH Zurich supervised by Prof. Olga Sorkine-Hornung. My area of research lies in applying machine learning techniques, especially deep learning, to challenging image and geometry processing problems. During my PhD study, I have had the honour to work at the Advanced Innovation Center for Future Visual Entertainment (AICFVE) in Beijing Film Academy, the Imaging and Video group at Disney Research Zurich and the Creative Intelligence Lab at Adobe Research in Seattle.\nBorn and raised in Beijing, I went to Technische UniversitÃ¤t MÃ¼nchen in Germany for study, where I obtained Bachelor of Science with distinction in Electrical Engineering and Information Technology.\nAfterwards I went to ETH Zurich in Switzerland, where I completed with the Master program Robotics, Systems and Control with distinction.\n","date":1642723200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1642723200,"objectID":"f91bdc3427753dc0f7ea9e5496fbbd2c","permalink":"https://yifita.github.io/authors/yifanwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yifanwang/","section":"authors","summary":"I\u0026rsquo;m a fourth-year PhD student in the Interactive Geometry Lab at ETH Zurich supervised by Prof. Olga Sorkine-Hornung. My area of research lies in applying machine learning techniques, especially deep learning, to challenging image and geometry processing problems.","tags":null,"title":"Yifan Wang","type":"authors"},{"authors":["Yifan Wang","Lukas Rahmann","OlgaSorkine"],"categories":[],"content":" Overview video Motivation Method  Implicit Displacement Fields Networks Transferability   Results Future work  Overview video   Motivation How to best represent 3D geometry in neural networks? Objects we encounter in real life distinguish themselves with their intricate geometric details. Is there a compact way to capture these intricacies and allow efficient downstream tasks?\nImplicit neural networks, also known as coordinate-based networks, has gained a lot of attraction due to their theoretically infinite resolution. But in reality, due to the spectral bias of neural nets, high-frequency signals (surface details) still get lost.\nThe research community combat this issue focusing two aspects: spatial partition and frequency transform.\n   Spatial Partition Frequency Transform     Divides the holistic implicit function into many simpler ones located in cells of some spatial structures Transforms the signal passing through the network to a high-frequency via either periodic activation functions or Fourier Transformation   DLS1, NGLOD2 SIREN3, Position Encoding4   Incompact, memory demands grows cubically with the spatial resolution, may have issues at the cell boundary Hard to train, local minima    Method Implicit Displacement Fields    We decompose the 3D geometry into a smooth base surface, represented as a low-frequency signed distance function, and a continuous high-frequency implicit displacement field, which offsets the base iso-surface along the normal direction.\n ðŸ’¡ Frequency-based partition of the 3D geometry \u0026ndash; decompose the shape into a smooth base surface, represented as a low-frequency signed distance function, and a continuous high-frequency implicit displacement field, which offsets the base iso-surface along the normal direction.\n Our idea took inspiration from Displacement Mapping, a classic technique in Computer Graphics to model surface details. In this image below, the bumps on the sphere is added by offsetting samples of the sphere along the normal directions by a distance obtained (with interpolation) from the height map on the top-left.   source: https://www.yankodesign.com/images/design_news/2019/04/how-to-create-realistic-textures-with-displacement-maps-in-keyshot-8 \nDisplacement mapping is discrete and defined on the surface. So in this paper, we are tasked to extend the definition of displacement mapping to the continous $\\mathbb{R}^3$ domain. This is illustratively shown in the figure below (please check our paper for the formal definition and proofs).\n Implicit displacement field $d: \\mathbb{R}^3 \\rightarrow \\mathbb{R}$ maps two equi-isosurfaces $\\mathcal{S}_{\\tau}$ and $\\hat{\\mathcal{S}}_{\\tau}$; height map in the classic displacement mapping is a special case $\\tau_{0}$.\nEqui-isosurface mapping: $f\\left(\\mathbf{x}\\right) = \\hat{f}\\left(\\mathbf{x}+d\\mathbf{n}\\right)$ and $\\hat{f}\\left(\\hat{\\mathbf{x}}\\right) = f\\left(\\hat{\\mathbf{x}}+\\hat{d}\\mathbf{n}\\right)$.\n   Implicit displacement field in 1D.  Networks How do we separate the low-frequency base SDF and high-frequency implicit displacement field? We notice that this is at its core a frequency decomposition of the geometry. At the same time, we observe that the output signal\u0026rsquo;s frequency of SIRENs (networks with period activation functions) can be controlled easily with their frequency paramter $\\omega$ in the sine activation $x\\mapsto\\sin\\left(\\omega x\\right)$.\n ðŸ’¡ We separate the low-frequency base and high-frequency detail in an unsupervised manner by leveraging SIREN\u0026rsquo;s inherent frequency capacity, which can be modulated conveniently by the hyper-parameter $\\omega$ in the sine activation $x\\mapsto\\sin\\left(\\omega x\\right)$. In other words, we approximate base SDF with a low-frequency SIREN (e.g. $\\omega=15$) and the displacement field with a high-frequency SIREN (e.g. $\\omega=60$ ).\n Let\u0026rsquo;s denote the base SDF network as $\\mathcal{N}^{\\omega_B}$ and the displacement network as $\\mathcal{N}^{\\omega_D}$. The detailed SDF (the one we want to approximate) is composed the following steps:\n compute the normal direction $\\mathbf{n} = \\frac{\\nabla \\mathcal{N}^{\\omega_B}\\left(\\mathbf{x}\\right)}{\\lVert\\nabla \\mathcal{N}^{\\omega_B}\\left(\\mathbf{x}\\right)\\rVert}$, compute the displacement distance $\\hat{d} = \\mathcal{N}^{\\omega_D}\\left(\\mathbf{x}\\right)$, compute the SDF at the displaced position $f(\\mathbf{x}+\\hat{d}\\mathbf{n}) = \\mathcal{N}^{\\omega_B}\\left(\\mathbf{x}+\\hat{d}\\mathbf{n}\\right)$, which per our definition is equal to the detail SDF $\\hat{f}(\\mathbf{x})$.  Notice that since both $\\mathcal{N}^{\\omega_B}$ and $\\mathcal{N}^{\\omega_D}$ are included in the steps, we can train them together by either directly comparing $\\hat{f}(\\mathbf{x})$ with the ground truth SDF or indirectly by solving eikonal constraints with boundary conditions.\nIn the paper, we also introduced 3 other techniques that we embedded in the network and training to improve the result and training stability. Go check them out! ðŸ˜‰\nTransferability One advantage of using classic displacement mapping to create surface details is that once the mapping, known as surface parameterization, from the base surface to the height map is created, the displacement is independent of deformations of the base surface. We emulate this effect for IDF by replacing the coordinates input to the displacement net, which is obviously non-transferable as it\u0026rsquo;s bound to the global coordinate frame, with transferable features.\n  Comparison between transferable and non-transferable implicit displacement.  Results Let\u0026rsquo;s first look at the detail representation compared to other methods:   Quantitative evaluation. Note that NGLOD (LOD6) requires storing more than 300 times as many parameters as our model. \nHere\u0026rsquo;s some visual examples.    \nFor detail transfer:    \nFuture work We hope to explore further in the realm of implicit shape editting! I\u0026rsquo;m looking forward to your inputs and ideas for collaborations.\n  Chabra, Rohan, et al. \u0026ldquo;Deep local shapes: Learning local sdf priors for detailed 3d reconstruction.\u0026rdquo; European Conference on Computer Vision. Springer, Cham, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Takikawa, Towaki, et al. \u0026ldquo;Neural geometric level of detail: Real-time rendering with implicit 3D shapes.\u0026rdquo; arXiv preprint arXiv:2101.10994 (2021).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Sitzmann, Vincent, et al. \u0026ldquo;Implicit neural representations with periodic activation functions.\u0026rdquo; Advances in Neural Information Processing Systems 33 (2020).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Mildenhall, Ben, et al. \u0026ldquo;Nerf: Representing scenes as neural radiance fields for view synthesis.\u0026rdquo; European Conference on Computer Vision. Springer, Cham, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1642723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642723200,"objectID":"062422059ff43d93e298fa769a177216","permalink":"https://yifita.github.io/publication/idf/","publishdate":"2021-06-09T00:00:00Z","relpermalink":"/publication/idf/","section":"publication","summary":"We extend classic displacement mapping to the neural implicit framework. The resulting novel implicit representation demonstrates superior reconstruction accuracy, parameter efficiency and enable implicit shape editing such as detail transfer.","tags":["surface reconstruction","deep learning","neural implicit","shape modeling"],"title":"Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields","type":"publication"},{"authors":["Yifan Wang","Carl Doesch","Relja ArandjeloviÄ‡","Joao Careira","Andrew Zisserman"],"categories":null,"content":"","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"128d6433693c59fe605529d4cdffab81","permalink":"https://yifita.github.io/publication/iib/","publishdate":"2021-12-10T00:00:00Z","relpermalink":"/publication/iib/","section":"publication","summary":"We incorporate inductive biases useful for multiple view geometry into this generalist model without having to touch its architecture, by instead encoding them directly as additional inputs.","tags":["deep learning","depth estimation","transformer","large scale"],"title":"Input-level Inductive Biases for 3D Reconstruction","type":"publication"},{"authors":["Ayush Tewari","Justus Thies","Ben Mildenhall","Pratul Srinivasan","Edgar Tretschk","Yifan Wang","Christoph Lassner","Vincent Sitzmann","Ricardo Martin-Brualla","Stephen Lombardi","Tomas Simon","Christian Theobalt","Matthias Niessner","Jonathan T Barron","Gordon Wetzstein","Michael Zollhoefer","Vladislav Golyanik"],"categories":null,"content":"","date":1636502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636502400,"objectID":"9fc200178eeb3364f42b6a834185d4bc","permalink":"https://yifita.github.io/publication/neural_rendering_review/","publishdate":"2021-12-10T00:00:00Z","relpermalink":"/publication/neural_rendering_review/","section":"publication","summary":"A review paper about the state-of-the-art in neural rendering.","tags":["neural rendering","review"],"title":"Advances in Neural Rendering","type":"publication"},{"authors":["ChristopherSchroers","Yifan Wang","FedericoPerazzi","BrianMcWilliams","AlexanderHornung"],"categories":null,"content":"","date":1623196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623196800,"objectID":"e00834c983415ad2d5405ef7a9d8c25a","permalink":"https://yifita.github.io/publication/pro_sr_patent/","publishdate":"2021-06-09T00:00:00Z","relpermalink":"/publication/pro_sr_patent/","section":"publication","summary":"According to one implementation, a video processing system includes a computing platform having a hardware processor and a system memory storing a software code including an artificial neural network (ANN). The hardware processor is configured to execute the software code to receive a first video sequence having a first display resolution, and to produce a second video sequence based on the first video sequence using the ANN. The second video sequence has a second display resolution higher than the first display resolution. The ANN is configured to provide sequential frames of the second video sequence that are temporally stable and consistent in color to reduce visual flicker and color shifting in the second video sequence.","tags":["image processing","deep learning","super-resolution","2D"],"title":"Video Super-Resolution Using An Artificial Neural Network","type":"publication"},{"authors":["Yifan Wang","ShihaoWu","CengizOeztireli","OlgaSorkine"],"categories":null,"content":"      We propose a hybrid neural surface representation with implicit functions and iso-points. The representation leads to accurate and robust surface reconstruction from imperfect data. The on-the-fly conversion with efficient iso-points extraction allows us to augment existing optimization pipelines in a variety of ways. In the first row, geometry-aware regularizers are incorporated to reconstruct a surface from a noisy point cloud; in the second row, geometric details are preserved in multi-view reconstruction via feature-aware sampling; in the last row, iso-points serve as a 3D prior to improve the topological accuracy of the reconstructed surface\n   We efficiently extract a dense, uniformly distributed set of iso-points as an explicit representation for a neural implicit surface. Since the extraction is fast, iso-points can be integrated back into the optimization as a 3D geometric prior, enhancing the optimization.\n","date":1615334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615334400,"objectID":"15cd37f60d23cdc2872794b443b4b736","permalink":"https://yifita.github.io/publication/iso_points/","publishdate":"2020-12-14T00:00:00Z","relpermalink":"/publication/iso_points/","section":"publication","summary":"Inter-dependent explicit representations for optimizing neural implicit surfaces","tags":["geometry processing","surface reconstruction","deep learning","neural implicit"],"title":"Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations","type":"publication"},{"authors":["CengizOeztireli","OlgaSorkine","ShihaoWu","Yifan Wang"],"categories":null,"content":"","date":1614816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614816000,"objectID":"953ebdeafc1580b34e8b04cfa68393ca","permalink":"https://yifita.github.io/publication/dss_patent/","publishdate":"2021-03-04T00:00:00Z","relpermalink":"/publication/dss_patent/","section":"publication","summary":"","tags":["geometry processing","point cloud","deep learning","rendering","3D"],"title":"Techniques for performing point-based inverse rendering","type":"publication"},{"authors":["ChristopherSchroers","Yifan Wang","Victor CornillÃ¨re","OlgaSorkine","AbdelazizDjelouah"],"categories":null,"content":"","date":1613606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613606400,"objectID":"a017a02139eb75dafa6725f3f578d369","permalink":"https://yifita.github.io/publication/blindsr_patent/","publishdate":"2021-12-10T00:00:00Z","relpermalink":"/publication/blindsr_patent/","section":"publication","summary":"","tags":["image processing","super-resolution","deep learning","image"],"title":"Techniques for upscaling images generated with undetermined downscaling kernels","type":"publication"},{"authors":[],"categories":null,"content":"","date":1612364400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612364400,"objectID":"11290c75e51b80b52aa7a719d49e3471","permalink":"https://yifita.github.io/talk/detail-driven-3d-content-creation/","publishdate":"2021-02-03T15:00:00Z","relpermalink":"/talk/detail-driven-3d-content-creation/","section":"event","summary":"A talk summarizing pretty much everything I've done in my IGL PhD.","tags":["geometry processing","surface reconstruction","deep learning","neural implicit","point cloud","super-resolution"],"title":"Detail-Driven 3D Content Creation","type":"event"},{"authors":[],"categories":null,"content":"","date":1591909200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591909200,"objectID":"1e865ae1c78805ea1f96800e3e5502c9","permalink":"https://yifita.github.io/talk/pursuing-high-resolution-3d-geometry-with-deep-learning/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/talk/pursuing-high-resolution-3d-geometry-with-deep-learning/","section":"event","summary":"Geometric details in 3D shapes is a defining factor in many industries such as AR/VR, VFX and design. However, creating high-fidelity shapes with fine-grained geometry details is a laborious process, requiring skillful artistry. Recently, many works have proposed different solutions for shape generations, but all of them fall short in constructing the high-frequency geometry details. In this talk, I'll introduce three recent works, which pursue to improve geometric details with Deep methods.","tags":["geometry processing","deep learning","point cloud","super-resolution","shape modeling","3d"],"title":"Pursuing high-resolution 3D Geometry with Deep Learning","type":"event"},{"authors":["Yifan Wang","NoamAigerman","VovaKim","SidChaudhuri","OlgaSorkine"],"categories":null,"content":"We can warp an arbitrary input shape to match the grob structure of an arbitrary target shape, while preserving all the local geometric details. The input and target shape is not required to have dense correspondences, the same topology, or even the same representation form (e.g. points, mesh and 2D image).    The key of our method is reducing the degrees of freedom of the deformation space by extending the traditional cage-based deformation technique.\nApplication 1 - Shape synthesis We use our method to learn a meaningful deformation space over a collection of shapes within the same category, and then use random pairs of source and target shapes to synthesize plausible variations of artist-generated assets.   Application 2 - Deformation transfer    We first learn the cage deformation space for a template source shape (top left) with known pose and body shape variations. Then, we annotate predefined landmarks on new characters in neutral poses (left column, rows 2-4). At test time, given novel target poses (top row, green) without known correspondences to the template, we transfer their poses to the other characters (blue).\nDue to the agnostic nature of cage-deformations to the underlying shape, we are able to seamlessly combine machine learning and traditional geometry processing to generalize to never-observed characters, even if the novel source and target characters are morphologically very different from the both the template source.\nTalk   ","date":1584144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584144000,"objectID":"56b48b416e5155ffbcd3738365bf7ef5","permalink":"https://yifita.github.io/publication/deep_cage/","publishdate":"2020-03-14T00:00:00Z","relpermalink":"/publication/deep_cage/","section":"publication","summary":"We propose a novel learnable representation for detail-preserving shape deformation extending a traditional cage-based deformation technique. We demonstrate the utility of our method for synthesizing shape variations and deformation transfer.","tags":["deformation","shape modeling","deep learning","3D"],"title":"Neural Cages for Detail-Preserving 3D Deformations","type":"publication"},{"authors":["Victor CornillÃ¨re","AbdelazizDjelouah","Yifan Wang","OlgaSorkine","ChristopherSchroers"],"categories":null,"content":"","date":1568073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568073600,"objectID":"5644e904519719091fad771448f41cd1","permalink":"https://yifita.github.io/publication/variational_blindsr/","publishdate":"2019-09-10T00:00:00Z","relpermalink":"/publication/variational_blindsr/","section":"publication","summary":"We propose a novel approach for single image super-resolution to simultaneously predict high resolution images and the degradation kernels.","tags":["image processing","super-resolution","deep learning","image"],"title":"Blind Image Super-Resolution with Spatially Variant Degradations","type":"publication"},{"authors":["Yifan Wang","Felice Serena","ShihaoWu","CengizOeztireli","OlgaSorkine"],"categories":null,"content":"   Shape Synthesis We can synthesize shapes from multiple 2D images. This process is not constrained by topology changes.   Point Cloud Filering Using DSS we can directly apply image-based filters to a point cloud to achieve various geometric effect.   Point Cloud Denoising We create state-of-the-art point cloud denoising results by marrying our differential renderer with the famous image-to-image translation deep learning framework Pix2Pix.   Accompanying Video   Acknowledgement We would like to thank Federico Danieli for the insightful discussion, Philipp Herholz for the timely feedack, Romann Weber for the video voice-over and Derek Liu for the help during the rebuttal. This work was supported in part by gifts from Adobe, Facebook and Snap, Inc.\n","date":1564358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564358400,"objectID":"f9cfe2faadb93867e711425faa964bd3","permalink":"https://yifita.github.io/publication/dss/","publishdate":"2019-07-29T00:00:00Z","relpermalink":"/publication/dss/","section":"publication","summary":"We propose a high-fidelity differentiable renderer for point clouds. We demonstrate how the proposed technique can be used to leverage contemporary deep neural networks to achieve state-of-the-art results in challenging geometry processing tasks.","tags":["geometry processing","point cloud","deep learning","rendering","3D"],"title":"Differentiable Surface Splatting for Point-based Geometry Processing","type":"publication"},{"authors":null,"categories":null,"content":"","date":1561766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561766400,"objectID":"80bb9a74f5dcab69055a451248709b85","permalink":"https://yifita.github.io/project/neural-shape/","publishdate":"2019-06-29T00:00:00Z","relpermalink":"/project/neural-shape/","section":"project","summary":"Representing and generating shapes using neural networks","tags":["Deep Learning","shape modeling","3D"],"title":"Neural Shapes","type":"project"},{"authors":["Yifan Wang","ShihaoWu","HuiHuang","DanielCohenor","OlgaSorkine"],"categories":null,"content":"   Results  $16\\times$ upsampling from 625 points     $16\\times$ upsampling from 5000 points    $16\\times$ upsampling from scan data    $16\\times$ upsampling from virtual scan data   References PU-Net: L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, \u0026ldquo;Pu-net: Point cloud upsampling network\u0026rdquo;, CVPR 2018\nEC-Net: L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, \u0026ldquo;Ec-net: an edge-aware point set consolidation network\u0026rdquo;, ECCV 2018\nEAR: H. Huang, S. Wu, M. Gong, D. Cohen-Or, U. Ascher, and H. Zhang, \u0026ldquo;Edge-aware point set resampling\u0026rdquo;, ACM ToG 2013\nWLOP: H. Huang, D. Li, H. Zhang, U. Ascher, and D. Cohen-Or, \u0026ldquo;Consolidation of unorganized point clouds for surface reconstruction\u0026rdquo;, SIGGRAPH Asia 2009\n","date":1561334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561334400,"objectID":"ad39d116047628dce95aceeb6af0c9ce","permalink":"https://yifita.github.io/publication/3pu/","publishdate":"2019-06-24T00:00:00Z","relpermalink":"/publication/3pu/","section":"publication","summary":"We present a detail-driven deep neural network for point set upsampling. A high-resolution point set is essential for point-based rendering and surface reconstruction. Inspired by the recent success of neural image super-resolution techniques, we progressively train a cascade of patch-based upsampling networks on different levels of detail end-to-end. We propose a series of architectural design contributions that lead to a substantial performance boost. The effect of each technical contribution is demonstrated in an ablation study. Qualitative and quantitative experiments show that our method significantly outperforms the state-of-the-art learning-based and optimazation-based approaches, both in terms of handling low-resolution inputs and revealing high-fidelity details.","tags":["point cloud","super-resolution","deep learning","geometry processing","3D"],"title":"Patch-base progressive 3D Point Set Upsampling","type":"publication"},{"authors":null,"categories":null,"content":"","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"45275d342533c1f0633acc26122b3285","permalink":"https://yifita.github.io/project/point-geometry/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/project/point-geometry/","section":"project","summary":"Making use of this extremely flexible yet unstructured form of shape represenation.","tags":["Deep Learning","geometry processing","point cloud","3D"],"title":"Point-based geometry processing","type":"project"},{"authors":["Yifan Wang","FedericoPerazzi","BrianMcWilliams","AlexanderHornung","OlgaSorkine","ChristopherSchroers"],"categories":null,"content":"   Results     MsLapSRN: Lai, Wei-Sheng, et al. \u0026ldquo;Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks.\u0026rdquo; arXiv preprint arXiv:1710.01992 (2017).\nEDSR, MDSR: Lim, Bee, et al. \u0026ldquo;Enhanced deep residual networks for single image super-resolution.\u0026rdquo; The IEEE CVPR (CVPR) Workshops. Vol. 1. No. 2. 2017.\nRDN:Zhang, Yulun, et al. \u0026ldquo;Residual Dense Network for Image Super-Resolution.\u0026rdquo; The IEEE CVPR (CVPR). 2018.\n   -- Featured Video It\u0026rsquo;s a pleasure to be featured in \u0026ldquo;2 minute paper\u0026rdquo;, an amazing YouTube channel that introduces latest development of AI in a variety of applications. Here\u0026rsquo;s the video that talks about our work.   ","date":1523836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523836800,"objectID":"2375f889a32d2d73dc112f3f464731d5","permalink":"https://yifita.github.io/publication/prosr/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/prosr/","section":"publication","summary":"We set a new benchmark for single-image super-resolution by exploiting progressiveness both in architecture and training. The proposed multi-scale models, **ProSR** and **ProSRGan**, improve the reconstruction quality in terms of PSNR and visual quality respectively. ProSR is one of the winning teams.","tags":["super-resolution","image processing","deep learning","image"],"title":"A Fully Progressive Approach to Single-Image Super-Resolution","type":"publication"},{"authors":null,"categories":null,"content":"","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"105797dda5efbb8aea73eb8475651802","permalink":"https://yifita.github.io/project/single-image-super-resolution/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/project/single-image-super-resolution/","section":"project","summary":"Pushing the limit of \"zoom-in\"s.","tags":["image processing","deep learning","super-resolution","2D"],"title":"Single-Image Super-Resolution","type":"project"},{"authors":["Yifan Wang","JieSong","Limin Wang","Luc Van Gool","OtmarHilliges"],"categories":null,"content":"Network Architecture    ","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467331200,"objectID":"766a3c831d4b11b535c5476c69fcdb9f","permalink":"https://yifita.github.io/publication/action_srcnn/","publishdate":"2016-07-01T00:00:00Z","relpermalink":"/publication/action_srcnn/","section":"publication","summary":"We propose a new deep architecture by incorporating object/human detection results into the framework for action recognition, called two-stream semantic region based CNNs (SR-CNNs). We perform experiments on UCF101 dataset and demonstrate its superior performance to the original two-stream CNNs. ","tags":["action recognition","video"],"title":"Two-Stream SR-CNNs for Action Recognition in Videos","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://yifita.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]