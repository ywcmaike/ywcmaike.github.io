<html>

<head>
<title>Weicai Ye's Homepage</title>
<style type="text/css" media="screen">
html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p,
blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em,
font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt,
dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot,
thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration: none;
}

a:focus,
a:hover {
  color: #f09228;
  text-decoration: none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h4 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
}

ul {
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

strong,
b {
  font-weight: bold;
}

em,
i {
  font-style: italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.3em;
  margin-bottom: 0.3em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-right: 230px;
}

div.paper div.wide {
  padding-right: 0px;
}

img.paper {
  margin-bottom: 0.5em;
  float: right;
  width: 200px;
}

span.blurb {
  font-style: italic;
  display: block;
  margin-top: 0.75em;
  margin-bottom: 0.5em;
}

span.news {
  display: block;
  margin-top: 0.5em;
}

pre,
code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-111088147-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

<script type="text/javascript" src="js/hidebib.js"></script>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
</head>

<body>

<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
  <div style="margin: 0px auto; width: 100%">
    <img title="Weicai Ye (叶伟才) " style="float: left; padding-left: 4em; height: 130px;" src="yeweicai.jpg">
    <div style="padding-left: 20em; vertical-align: top; height: 100px;">
      <span style="line-height: 300%; font-size: 16pt;">Weicai Ye 叶伟才 </span><br />
      <span>Ph.D. Candidate</span><br />
      <span>3D Vision Group, State Key Lab of CAD&CG</span><br />
      <span>College of Computer Science, Zhejiang University</span><br />
      <!-- <span><a href="https://github.com/zju3dv/" target="_blank">3D Vision Group</a>, <a href="http://www.zjucvg.net/" target="_blank">State Key lab of CAD&CG</a></span><br /> -->
      <!-- <span><a href="http://www.cs.zju.edu.cn/" target="_blank">College of Computer Science, Zhejiang University</a></span><br /> -->

      <a href="mailto:yeweicai@zju.edu.cn">Email</a> /
      <!-- <a onclick="_gaq.push(['_trackEvent', 'Personal', 'Download', 'CV']);" href="https://drive.google.com/open?id=1YYLEhIkchrcSeBanGsbH5snCZULFwyCJ" target="_blank">CV</a> / -->
      <!-- <a href="https://scholar.google.com/citations?hl=en&user=0LeXf0YAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Scholar</a> / -->
      <a href="https://github.com/ywcmaike" target="_blank">Github</a> /
      <a href="https://www.linkedin.com/in/weicai-ye-b9b36b129/" target="_blank">LinkedIn</a> /
      <a href="https://scholar.google.com/citations?hl=zh-CN&user=qsMRsnsAAAAJ" target="_blank">Google Scholar</a> /
      <a href="WeicaiYe.pdf">CV</a>
    </div>
  </div>
</div>

<div style="clear: both;"><!-- page div -->

<div class="section">
  <h2>About</h2>
  <div class="paper">
    <p>I am a third year (2018-) Ph.D. candidate in Computer Science at <a href="http://www.zju.edu.cn/english/" target="_blank">Zhejiang University</a>, supervised by <a href="http://www.cad.zju.edu.cn/home/bao/" target="_blank">Prof. Hujun Bao</a> and <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Prof. Guofeng Zhang</a>. I obtained my bachelor's degree in Computer Software Engineering from <a href="http://en.uestc.edu.cn/" target="_blank">University of Electronic Science and Technology of China (UESTC)</a> in 2018.
      <!-- , where I ranked 3rd of the 111 students in major and got National Encouragement Scholarship.  -->
    <!-- I interned at <a href="https://koolab.kujiale.com" target="_blank">KooLab</a> (summer 2019). -->
    </p>
    <br>
    <p>
    I am interested in <em>computer vision</em> and <em>machine learning</em>, particularly in <strong>3D Vision</strong>. My research goal is to make computers (<strong>robotics</strong>) learn to perceive, localize, reconstruct, reason, and interact with the real world. My current research focuses on <strong>Visual Localization, 3D Reconstruction, Mixed Reality, Scene Understanding and Video Segmentation</strong>.
    </p>
  </div>
</div>

<div class="section">
  <h2>News</h2>
  <div class="paper" id="news">
    <ul>
      <li><b>[07/2021]</b> One paper is accepted to <a href="https://www.iros2021.org/" target="_blank">IROS 2021</a>.</li>
      <li><b>[01/2021]</b> One paper is accepted to <a href="http://ieeevr.org/2021/program/papers/" target="_blank">VR 2021</a>.</li>
      <li><b>[10/2020]</b> One paper is accepted to <a href="http://3dv2020.dgcv.nii.ac.jp/index.html" target="_blank">3DV 2020</a>.</li>
      <li><b>[08/2020]</b> Our team won 5th in <a href="https://www.biendata.xyz/competition/gigavision/final-leaderboard/" target="_blank">ECCV GigaVision Challenge 2020</a>.</li>
      <li><b>[07/2020]</b> One paper is accepted to <a href="http://ismar20.org/" target="_blank">ISMAR 2020</a>.</li>
      
<!--      <li><b>[08/2019]</b> We release a large photo-realistic dataset, <a href="http://structured3d-dataset.org" target="_blank">Structured3D dataset</a>, for data-driven structured 3D reconstruction!</li>-->
<!--      <li><b>[02/2019]</b> Three papers are accepted to <a href="http://cvpr2019.thecvf.com" target="_blank">CVPR 2019</a>.</li>-->
<!--      <a shape="rect" href="javascript:toggleabs('news')" class="toggleabs">Archive</a>-->
<!--      <span class="news">-->
<!--      <li><b>[06/2019]</b> Our paper on planar reconstruction is invited to be presented at the <a href="https://3dscenegen.github.io/" target="_blank">3D Scene Generation Workshop</a> at <a href="http://cvpr2019.thecvf.com" target="_blank">CVPR 2019</a>.</li>-->
<!--      <li><b>[07/2017]</b> Second place of <a href="https://www.vision.ee.ethz.ch/webvision/2017/workshop.html" target="_blank">WebVision Challenge</a> on Image Classification Task (Top University Team).</li>-->
      </span>
    </ul>
  </div>
</div>

<!-- - TEMPLATE
<div class="paper" id="X">
  <div>
    <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'X-eccv-14']);" href="papers/X.pdf">X</a><br />
    <strong>Ross Girshick</strong> <br /> European Conference on Computer Vision (ECCV), 2014<br />
    <a shape="rect" href="javascript:toggleabs('X')" class="toggleabs">abstract</a> /
    <a shape="rect" href="javascript:togglebib('X')" class="togglebib">bibtex</a>
    <pre xml:space="preserve">
    @inproceedings{X,
        Author    = {X},
        Title     = {X},
        Booktitle = {Proceedings of the European
                     Conference on Computer Vision ({ECCV})},
        Year      = {2014}}
    </pre>
    <span class="blurb">X</span>
  </div>
  <div class="spanner"></div>
</div>
-->

<div class="section">
  <h2 id="reports">Publications</h2>
  <div class="paper" id="ISMAR2020">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'ISMAR2020']);" href="https://arxiv.org/abs/1908.00222"> -->
      <a href="https://ieeexplore.ieee.org/document/9284705">
        Learning Bipartite Graph Matching for Camera Localization
      </a><br />
      Hailin Yu, <strong>Weicai Ye</strong>, Youji Feng, Hujun Bao, Guofeng Zhang<br />
      <strong>IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 2020</strong> <br />
      <!-- <span>
        2D-3D matching is an essential step for visual localization, where the
accuracy of the camera pose is mainly determined by the quality of
2D-3D correspondences. The matching is typically achieved by the
nearest neighbor search of local features. Many existing works have
shown impressive results on both efficiency and accuracy. Recently
emerged learning-based features further improve the robustness
compared to the traditional hand-crafted ones. However, it is still
not easy to establish enough correct matches in challenging scenes
with illumination changes or repetitive patterns due to the intrinsic
local property of the features. In this work, we propose a novel
method to deal with 2D-3D matching in a very robust way. We
first establish as many potential correct matches as possible using
the local similarity. Then we construct a bipartite graph and use a
convolutional neural network, referred to as Bipartite Graph Network
(BGNet), to extract the global geometric information. The network
predicts the likelihood of being an inlier for each edge and outputs
the globally optimal one-to-one correspondences with a Hungarian
pooling layer. The experiments show that the proposed method is
able to find more correct matches, and improve localization in both
robustness and accuracy. The results on multiple visual localization
datasets are obviously better than the existing state-of-the-arts, which
demonstrate the effectiveness of the proposed method.      </span> <br/> -->
<a href="https://docs.google.com/presentation/d/1fEzsb3RmZIa8kfDGopNcZ62agY5FuTeQJu3xj9XjNrM/mobilepresent?slide=id.ga6f4ee1874_0_1"> poster </a> <br/>
      <!-- <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'ZhengZLTGZ19']);" href="https://drive.google.com/file/d/17F_jIfY_QKFNmsOSvzUFZwWKrr6YUMnQ" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Webpage', 'Download', 'ZhengZLTGZ19']);" href="https://structured3d-dataset.org" target="_blank">webpage</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'ZhengZLTGZ19']);" href="https://github.com/bertjiazheng/Structured3D" target="_blank">code</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Zhihu', 'Download', 'ZhengZLTGZ19']);" href="https://zhuanlan.zhihu.com/p/77555645" target="_blank">zhihu blog (chinese)</a> /
      <a shape="rect" href="javascript:togglebib('ZhengZLTGZ19')" class="togglebib">bibtex</a><br /> -->
    <!-- <a > video </a> <a> code </a>    -->
   <!--    <pre xml:space="preserve">
@article{ZhengZLTGZ19,
  author    = {Jia Zheng and
               Junfei Zhang and
               Jing Li and
               Rui Tang and
               Shenghua Gao and
               Zihan Zhou},
  title     = {Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling},
  journal   = {CoRR},
  volume    = {abs/1908.00222},
  year      = {2019}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>
  <div class="paper" id="3DV2020">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'IJCAI2020']);"> -->
      <a href="https://ieeexplore.ieee.org/document/9320412">
        Saliency Guided Subdivision for Single-View Mesh Reconstruction
      </a><br />
      Hai Li*, <strong>Weicai Ye*</strong>, Guofeng Zhang, Sanyuan Zhang, Hujun Bao (* equal contribution) <br /> 
      <!-- <a href="http://cvpr2020.thecvf.com/" target="_blank">CVPR 2020</a> / -->
<!--      MM2020 -->
      <strong>International Conference on 3D Vision (3DV), 2020</strong> <br />
      <a href="3dv2020_poster.pdf"> poster </a> / <a href="https://youtu.be/fgxplMU0H3M"> video </a> </br>
      <!-- / <a href="https://github.com/garylidd/pyg_model"> code </a> </br> -->
      <!-- <span>
        In this paper, we present a novel deep architecture to recover a 3D shape in triangular mesh from a single image based on mesh deformation. Most existing deformation-based methods produce uniform mesh predictions by repeatedly applying global subdivision but fail to require the highlighted details due to the memory limits. To address this problem, we propose a novel saliency guided subdivision method to achieve the trade-off between detail generation and memory consumption. Instead of using local geometric cues such as curvature, we introduce a global point-based saliency voting operation to guide the adaptive mesh subdivision and deformation explicitly. 
Moreover, we propose the oriented chamfer loss to mitigate the mesh self-intersection problem in subdivision. We further make our network configurable and explore the best structure combination. Extensive experiments show that our method can both produce visually pleasing results with fine details and achieve better performance compared to other state-of-the-art methods.      </span> -->
<!--       <a shape="rect" href="javascript:togglebib('JinXZZTXYG20')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{JinXZZTXYG20,
  author    = {Lei Jin and
               Yanyu Xu and
               Jia Zheng and
               Junfei Zhang and
               Rui Tang and
               Shugong Xu and
               Jingyi Yu and
               Shenghua Gao},
  title     = {Geometric Structure Based and Regularized Depth Estimation From
               360 Indoor Imagery},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  year      = {2020}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>
  

  <!--------------------------------------------------------------------------->
  <!--YuZLZG19-->
  <!--------------------------------------------------------------------------->
  <div class="paper" id="2020">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'MM2020']);" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Single-Image_Piece-Wise_Planar_3D_Reconstruction_via_Associative_Embedding_CVPR_2019_paper.pdf"> -->
      <a href="https://ieeexplore.ieee.org/abstract/document/9417783">
        SuperPlane: 3D Plane Detection and Description from a Single Image
      </a><br />
      <strong>Weicai Ye</strong>, Hai Li, Tianxiang Zhang, Xiaowei Zhou, Hujun Bao, Guofeng Zhang<br />
      <strong>IEEE Conference on Virtual Reality and 3D User Interfaces (VR), 2021 </strong> <br />
      <a href="https://youtu.be/cZW1YHuF-rM"> video </a>
      <!-- <span>
        We present a novel end-to-end plane detection and description network named SuperPlane to tackle the challenging conditions in matching problems. Our network takes a single image as input and extracts 3D planes and generates corresponding descriptors simultaneously. We also propose a mask-attention module and an instance-triplet loss to improve the distinctiveness of the plane descriptor. The detected plane descriptors are treated as discrete distributions and an Area-Aware Kullback-Leibler (KL) Divergence Retrieval method is proposed for the Image-Based Localization (IBL) task. Extensive experiments show that our method outperforms state-of-the-art methods and retains good generalization capacity. An AR application is presented to demonstrate the effectiveness of the proposed method.
      </span> -->
      <!-- <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'YuZLZG19']);" href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Yu_Single-Image_Piece-Wise_Planar_CVPR_2019_supplemental.pdf" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'YuZLZG19']);" href="https://github.com/svip-lab/PlanarReconstruction" target="_blank">code</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Poster', 'Download', 'YuZLZG19']);" href="https://drive.google.com/open?id=1th86ZZfoJ_6dLTcpOXfIpzUj7vfFVxMi" target="_blank">poster</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Slide', 'Download', 'YuZLZG19']);" href="https://drive.google.com/open?id=1YaEQ_PQg_H-0rDSrA1tDFSWOK8ZkGnBv" target="_blank">slide</a> /
      <a shape="rect" href="javascript:togglebib('YuZLZG19')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{YuZLZG19,
  author    = {Zehao Yu and
               Jia Zheng and
               Dongze Lian and
               Zihan Zhou and
               Shenghua Gao},
  title     = {Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  pages     = {1029--1037},
  year      = {2019}
}
</pre>
    </div>
    <div class="spanner"></div> -->
  </div>
    <div class="spanner"></div>
  </div>

  <div class="paper" id="2021">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'MM2020']);" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Single-Image_Piece-Wise_Planar_3D_Reconstruction_via_Associative_Embedding_CVPR_2019_paper.pdf"> -->
      <a>
        Coxgraph: Multi-Robot Collaborative, Globally Consistent, Online Dense Reconstruction System
      </a><br />
      Xiangyu Liu, <strong>Weicai Ye</strong>, Chaoran Tian, Zhaopeng Cui, Hujun Bao and Guofeng Zhang<br />
      <strong>2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021</strong> <br />
      <a href="https://youtu.be/F63OrZEXv4Y"> video </a>
      <!-- <span>
        We present a novel end-to-end plane detection and description network named SuperPlane to tackle the challenging conditions in matching problems. Our network takes a single image as input and extracts 3D planes and generates corresponding descriptors simultaneously. We also propose a mask-attention module and an instance-triplet loss to improve the distinctiveness of the plane descriptor. The detected plane descriptors are treated as discrete distributions and an Area-Aware Kullback-Leibler (KL) Divergence Retrieval method is proposed for the Image-Based Localization (IBL) task. Extensive experiments show that our method outperforms state-of-the-art methods and retains good generalization capacity. An AR application is presented to demonstrate the effectiveness of the proposed method.
      </span> -->
      <!-- <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'YuZLZG19']);" href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Yu_Single-Image_Piece-Wise_Planar_CVPR_2019_supplemental.pdf" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'YuZLZG19']);" href="https://github.com/svip-lab/PlanarReconstruction" target="_blank">code</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Poster', 'Download', 'YuZLZG19']);" href="https://drive.google.com/open?id=1th86ZZfoJ_6dLTcpOXfIpzUj7vfFVxMi" target="_blank">poster</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Slide', 'Download', 'YuZLZG19']);" href="https://drive.google.com/open?id=1YaEQ_PQg_H-0rDSrA1tDFSWOK8ZkGnBv" target="_blank">slide</a> /
      <a shape="rect" href="javascript:togglebib('YuZLZG19')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{YuZLZG19,
  author    = {Zehao Yu and
               Jia Zheng and
               Dongze Lian and
               Zihan Zhou and
               Shenghua Gao},
  title     = {Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  pages     = {1029--1037},
  year      = {2019}
}
</pre>
    </div>
    <div class="spanner"></div> -->
  </div>
    <div class="spanner"></div>
  </div>

  <div class="paper" id="2021">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'MM2020']);" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Single-Image_Piece-Wise_Planar_3D_Reconstruction_via_Associative_Embedding_CVPR_2019_paper.pdf"> -->
      <a>
        ARCargo: Multi-Device Integrated Cargo Loading Management System with Augmented Reality
      </a><br />
      Tianxiang Zhang, Chong Bao, Hongjia Zhai, Jiazhen Xia, <strong>Weicai Ye*</strong>, and Guofeng Zhang (* indicates project manager)<br />
      <strong>In submission</strong> <br />
      <a href="https://youtu.be/La7TNMIDWvY"> video </a>
      <!-- <span>
        We present a novel end-to-end plane detection and description network named SuperPlane to tackle the challenging conditions in matching problems. Our network takes a single image as input and extracts 3D planes and generates corresponding descriptors simultaneously. We also propose a mask-attention module and an instance-triplet loss to improve the distinctiveness of the plane descriptor. The detected plane descriptors are treated as discrete distributions and an Area-Aware Kullback-Leibler (KL) Divergence Retrieval method is proposed for the Image-Based Localization (IBL) task. Extensive experiments show that our method outperforms state-of-the-art methods and retains good generalization capacity. An AR application is presented to demonstrate the effectiveness of the proposed method.
      </span> -->
      <!-- <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'YuZLZG19']);" href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Yu_Single-Image_Piece-Wise_Planar_CVPR_2019_supplemental.pdf" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'YuZLZG19']);" href="https://github.com/svip-lab/PlanarReconstruction" target="_blank">code</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Poster', 'Download', 'YuZLZG19']);" href="https://drive.google.com/open?id=1th86ZZfoJ_6dLTcpOXfIpzUj7vfFVxMi" target="_blank">poster</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Slide', 'Download', 'YuZLZG19']);" href="https://drive.google.com/open?id=1YaEQ_PQg_H-0rDSrA1tDFSWOK8ZkGnBv" target="_blank">slide</a> /
      <a shape="rect" href="javascript:togglebib('YuZLZG19')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{YuZLZG19,
  author    = {Zehao Yu and
               Jia Zheng and
               Dongze Lian and
               Zihan Zhou and
               Shenghua Gao},
  title     = {Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  pages     = {1029--1037},
  year      = {2019}
}
</pre>
    </div>
    <div class="spanner"></div> -->
  </div>
    <div class="spanner"></div>
  </div>

  


<div class="paper" id="2021">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'ISMAR2020']);" href="https://arxiv.org/abs/1908.00222"> -->
      <a>
        Real-time and Efficient Dense Correspondence Network
      </a><br />
      <strong>Weicai Ye</strong><br />
      <!-- Extra experiments have been conducted.  <br /> -->
      <span>
        Propose a light-weight and efficient architecture for finding dense correspondence in real-time.    </span> <br/>
        <strong>In submission</strong> <br />
        <!-- <li><strong>the baseline(reimplementation of STM) Ranked 33th among the 702 teams in the MEDIA AI Alibaba Entertainment Algorithm Challenge-High-precision video character segmentation track </strong> </li> <br/> -->
      <!-- <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'ZhengZLTGZ19']);" href="https://drive.google.com/file/d/17F_jIfY_QKFNmsOSvzUFZwWKrr6YUMnQ" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Webpage', 'Download', 'ZhengZLTGZ19']);" href="https://structured3d-dataset.org" target="_blank">webpage</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'ZhengZLTGZ19']);" href="https://github.com/bertjiazheng/Structured3D" target="_blank">code</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Zhihu', 'Download', 'ZhengZLTGZ19']);" href="https://zhuanlan.zhihu.com/p/77555645" target="_blank">zhihu blog (chinese)</a> /
      <a shape="rect" href="javascript:togglebib('ZhengZLTGZ19')" class="togglebib">bibtex</a><br /> -->
   <!--    <pre xml:space="preserve">
@article{ZhengZLTGZ19,
  author    = {Jia Zheng and
               Junfei Zhang and
               Jing Li and
               Rui Tang and
               Shenghua Gao and
               Zihan Zhou},
  title     = {Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling},
  journal   = {CoRR},
  volume    = {abs/1908.00222},
  year      = {2019}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>

<div class="paper" id="2021">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'ISMAR2020']);" href="https://arxiv.org/abs/1908.00222"> -->
      <a>
        Real-Time and Efficient Video Panoptic Segmentation Network
      </a><br />
      <strong>Weicai Ye</strong><br />
      <!-- Extra experiments have been conducted.  <br /> -->
      <span>
        Propose a light-weight and efficient architecture for video panoptic segmentation in real-time. </span> <br/>
        <strong>In submission</strong> <br />
        <!-- <li><strong>the baseline(reimplementation of STM) Ranked 33th among the 702 teams in the MEDIA AI Alibaba Entertainment Algorithm Challenge-High-precision video character segmentation track </strong> </li> <br/> -->
      <!-- <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'ZhengZLTGZ19']);" href="https://drive.google.com/file/d/17F_jIfY_QKFNmsOSvzUFZwWKrr6YUMnQ" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Webpage', 'Download', 'ZhengZLTGZ19']);" href="https://structured3d-dataset.org" target="_blank">webpage</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'ZhengZLTGZ19']);" href="https://github.com/bertjiazheng/Structured3D" target="_blank">code</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Zhihu', 'Download', 'ZhengZLTGZ19']);" href="https://zhuanlan.zhihu.com/p/77555645" target="_blank">zhihu blog (chinese)</a> /
      <a shape="rect" href="javascript:togglebib('ZhengZLTGZ19')" class="togglebib">bibtex</a><br /> -->
   <!--    <pre xml:space="preserve">
@article{ZhengZLTGZ19,
  author    = {Jia Zheng and
               Junfei Zhang and
               Jing Li and
               Rui Tang and
               Shenghua Gao and
               Zihan Zhou},
  title     = {Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling},
  journal   = {CoRR},
  volume    = {abs/1908.00222},
  year      = {2019}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>

  <div class="paper" id="2021">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'ISMAR2020']);" href="https://arxiv.org/abs/1908.00222"> -->
      <a>
       Efficient PointCloud Instance Segmentation Network
      </a><br />
      <strong>Weicai Ye</strong><br />
      <!-- Extra experiments have been conducted.  <br /> -->
      <span>
        Propose a light-weight and efficient architecture for PointCloud instance segmentation. </span> <br/>
        <strong>In submission</strong> <br />
        <!-- <li><strong>the baseline(reimplementation of STM) Ranked 33th among the 702 teams in the MEDIA AI Alibaba Entertainment Algorithm Challenge-High-precision video character segmentation track </strong> </li> <br/> -->
      <!-- <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'ZhengZLTGZ19']);" href="https://drive.google.com/file/d/17F_jIfY_QKFNmsOSvzUFZwWKrr6YUMnQ" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Webpage', 'Download', 'ZhengZLTGZ19']);" href="https://structured3d-dataset.org" target="_blank">webpage</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'ZhengZLTGZ19']);" href="https://github.com/bertjiazheng/Structured3D" target="_blank">code</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Zhihu', 'Download', 'ZhengZLTGZ19']);" href="https://zhuanlan.zhihu.com/p/77555645" target="_blank">zhihu blog (chinese)</a> /
      <a shape="rect" href="javascript:togglebib('ZhengZLTGZ19')" class="togglebib">bibtex</a><br /> -->
   <!--    <pre xml:space="preserve">
@article{ZhengZLTGZ19,
  author    = {Jia Zheng and
               Junfei Zhang and
               Jing Li and
               Rui Tang and
               Shenghua Gao and
               Zihan Zhou},
  title     = {Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling},
  journal   = {CoRR},
  volume    = {abs/1908.00222},
  year      = {2019}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>

  

</div>

<div class="section">
  <h2 id="reports">Experience</h2>
  <div class="paper" id="Sensetime2018">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'IJCAI2020']);"> -->
      <a>
        3D Reconstruction of Indoor Scene of RGB-D Images
      </a><br />
      <strong>SenseTime Group Ltd.</strong>  Hangzhou, China,  Jan 2018 -- May 2018 <br/>
      3D Vision-Researcher Internship <br/>
      
      <span>
        Integrate traditional RGBD SLAM and semantic segmentation, plane detection to form SemanticSLAM. <br/>
        Different modules cooperate with each other to effectively improve the quality of localization and mapping. <br/>
        The system can perform real-time reconstruction of room-scale indoor scenes.
      </span>
<!--       <a shape="rect" href="javascript:togglebib('JinXZZTXYG20')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{JinXZZTXYG20,
  author    = {Lei Jin and
               Yanyu Xu and
               Jia Zheng and
               Junfei Zhang and
               Rui Tang and
               Shugong Xu and
               Jingyi Yu and
               Shenghua Gao},
  title     = {Geometric Structure Based and Regularized Depth Estimation From
               360 Indoor Imagery},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  year      = {2020}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>
  <div class="paper" id="Baidu2017">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'IJCAI2020']);"> -->
      <a>
        Video Search System
      </a><br />
      <strong> Baidu Inc. </strong> Beijing, China, Feb 2017 -- Jul 2017<br/>
      Software Engineer Internship <br />
      <span>
        Cooperate with colleagues to develop millisecond-level response video search services that can support hundreds of millions of highly concurrent retrieval needs. <br/>
        Cooperate with colleagues to develop rearrangement strategies such as video resolution, cross-modal fusion, etc to improve the quality of video retrieval results.<br/>
        The system can retrieve the related video with the given text in real time and the service is online.
      </span>
<!--       <a shape="rect" href="javascript:togglebib('JinXZZTXYG20')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{JinXZZTXYG20,
  author    = {Lei Jin and
               Yanyu Xu and
               Jia Zheng and
               Junfei Zhang and
               Rui Tang and
               Shugong Xu and
               Jingyi Yu and
               Shenghua Gao},
  title     = {Geometric Structure Based and Regularized Depth Estimation From
               360 Indoor Imagery},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  year      = {2020}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>
</div>

<div class="section">
  <h2>Awards & Honors</h2>
  <div class="paper">
    <ul>
      <!-- <li>Ranked 33th among the 702 teams in the MEDIA AI Alibaba Entertainment Algorithm Challenge-High-precision video character segmentation track --2020.07 </li> -->
      <li><strong>5th </strong> in ECCV GigaVision Challenge -- 2020 </li>
      <li><strong>6th </strong> among the 1945 teams in the Taobao Live Product Identification Contest --2020 </li>
      <li>Zhijun He Outstanding Scholarship                                       -- 2019 </li>
      <li>Chiang Chen Industrial Charity Foundation Grant                         -- 2019 </li>
      <li>Excellence Price of the 1st IKCEST "The Belt and Road" International Big Data Competition. --2019</li>
      <li><strong>Champion</strong> of 2018 Cloudwalk Headcount Challenge with 31,500¥ Bonus       -- 2019 </li>
      <li>Graduate Student Scholarship                                            -- 2018,2019,2020 </li>
      <li>National Encouragement Scholarship（Ranked 3rd of 111 Students）           -- 2017 </li>
      <li><strong>Meritorious Winner </strong> in Mathematical Contest Modeling                     -- 2017 </li>
      <li>First Prize in Sichuan Province Contest District in China Undergraduate Mathematical Contest in Modeling -- 2016 </li>
    </ul>
  </div>
</div>

<div class="section">
  <h2>Teaching</h2>
  <div class="paper">
    <ul>
      <li><a href="http://www.cad.zju.edu.cn/home/gfzhang/course/computational-photography/">Computational Photograph</a>, Teaching Assistant (TA)     - Spring 2021</li>
      
    </ul>
  </div>
</div>

</div><!-- close page div -->

<div style="clear:both;">
  <div style="padding-left: 1em; float: left; display: block;">
    Last updated: July 1st, 2021
  </div>
  <div style="padding-right: 1em; float: right; display: block;">
    Template by <a href="http://www.rossgirshick.info/">rbg</a>.
  </div>
  <br>
</div>

<script xml:space="preserve" language="JavaScript">
  hideallbibs();
  hideallabs();
</script>

</body>

</html>
