<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Weicai Ye Âè∂‰ºüÊâç</title>
  
  <meta name="author" content="Weicai Ye">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Weicai Ye Âè∂‰ºüÊâç</name>
              </p>
              <p>I am currently a Senior Researcher at Visual Generation Group (a.k.a. <a href="https://kling.kuaishou.com/en">Kling Team</a>), <a href="https://www.kuaishou.com/en">Kuaishou Technology</a>, working on Multimodal Generative Foundation Models and World Models.
                <!-- Before that, I interned at Shanghai AI Laboratory, working with <a href="https://tonghe90.github.io/">Dr. Tong He</a>, <a href="https://wlouyang.github.io/">Prof. Wanli Ouyang</a> and <a href="https://mmlab.siat.ac.cn/yuqiao">Prof. Yu Qiao</a>. -->
              </p>
              <p>Previously, I was a visiting researcher at ETH Zurich, advised by <a href="https://people.inf.ethz.ch/marc.pollefeys/">Prof. Marc Pollefeys</a>.
                I received my Ph.D. in 2024 from ZJU3DV, State Key Lab of CAD&CG, Zhejiang University, advised by <a href="http://www.cad.zju.edu.cn/home/bao/">Prof. Hujun Bao</a> and <a href="http://www.cad.zju.edu.cn/home/gfzhang/">Prof. Guofeng Zhang</a>. I obtained my bachelor's degree from UESTC in 2018.
              </p>
              <p>
                My research goal is to make computers (<strong>robotics</strong>) learn to perceive, localize, reconstruct, reason, and interact with the real world like humans, that is <strong>AGI</strong>. 
                I'm interested in <strong>Multimodal Video Generation, World Models, 3D Vision Foundation Models, and Embodied AI, especially correspondence, 3D/4D reconstruction, rendering, generation, and robotics manipulation</strong>.
              </p>
              <p> <font color="red">We are actively looking for research interns who have strong backgrounds in MLLM, Audio, Video Generation, 3D Vision, etc., to work on cutting-edge research topics. Feel free to email me if you are interested. </font>
              </p>

              <p style="text-align:center">
                <a href="mailto:maikeyeweicai@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="images/WeicaiYe_CV_clean.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="...">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=qsMRsnsAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://x.com/WeicaiYe">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/ywcmaike">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ywc.jpg"><img style="width:100%;max-width:70%" alt="profile photo" src="images/ywc.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <ul>
              <li>
                <div class="marker"><div>[2025-03] Invited by <a href="https://mp.weixin.qq.com/s/aq2Z-kq-z-x2tYWEc0URdA">VALSE Webinar</a>, I give a talk <a href="talks/multi-modal video generative foundation models.pdf">"Multi-modal Video Generative Foundation Models"</a> about our recent work <a href="https://fulldit.github.io/">FullDiT</a> and <a href="https://any2caption.github.io">Any2Caption</a>. </div>
            </li>
            <li>
              <div class="marker"><div>[2025-03] StreetSurfGS accepted to TCSVT 2025 (corresponding author).</div>
          </li>
              <li>
                <div class="marker"><div>[2025-02] Three papers accepted to CVPR 2025 (one corresponding author). </div>
            </li>
              <li>
                <div class="marker"><div>[2025-01] One paper accepted to ICRA 2025 and one paper accepted to RA-L 2025. </div>
            </li>
              <li>
                  <div class="marker"><div>[2025-01] Five papers (including <font color="red">2 Spotlight</font>) accepted to ICLR 2025 (100% acceptance rate, two corresponding author). Congratulations to all my collaborators!</div>
              </li>
              <li>
                <div class="marker"><div>[2025-01] Welcome to check out our <a href="https://klingai.kuaishou.com/image-to-video/multi-id/new">multi-reference images to video generation</a> in the Kling app/web. </div>
              </li>
              <li>
                <div class="marker"><div>[2024-12] GigaGS accepted to AAAI 2025 (corresponding author).</div>
            </li>
            <li>
              <div class="marker"><div>[2024-11] PGSR was accepted to TVCG 2024.</div>
          </li>
              <li>
                  <div class="marker"><div>[2024-10] Excited to join Kling Team, Kuaishou Technology, as a senior researcher.</div>
              </li>
              <li>
                  <div class="marker"><div>[2024-09] Defended my Ph.D. thesis with 5A rating.</div>
              </li>
              <li>
                  <div class="marker">[2024-09] Three papers accepted to NeurIPS 2024 (1 first-author and 1 corresponding author).</div>
              </li>
            </ul>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <!-- <p>
                I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
              </p> -->
              <p>* denotes equal contribution, ‚Ä† denotes corresponding author, ‚Ä° denotes project lead. 
                Representative works are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <!-- <tr onmouseout="matchanything_stop()" onmouseover="matchanything_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='matchanything_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/MatchAnything/overview.jpg" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/MatchAnything/overview.jpg' width="160">
              </div>
              <script type="text/javascript">
                function matchanything_start() {
                  document.getElementById('matchanything_image').style.opacity = "1";
                }
      
                function matchanything_stop() {
                  document.getElementById('matchanything_image').style.opacity = "0";
                }
                matchanything_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://matchanything.github.io/">
            <span class="papertitle">
              Match Anything: A Generic Correspondence Foundation Model
            </span>
              </a>
              <br>
              <strong>Weicai Ye‚Ä°</strong>,
              <a href="">Ruohao Zhan</a>,
              <a href="">Hujun Bao</a>,
              <a href="">Wanli Ouyang</a>,
              <a href="">Yu Qiao</a>,
              <a href="">Tong He‚Ä†</a>,
              <a href="">Guofeng Zhang‚Ä†</a>
              <br>
              <em>Arxiv</em> 2024 coming
              <br>
              <br>
              <a href="">project page</a>
              /
              <a href="">arXiv</a>
              /
                    <a href="">code</a>
              <p></p>
              <p>
                Proposed video correspondence foundation model with largest video correspondence dataset. 
                Unified three representations of correspondence learning: feature matching, optical flow, and point tracking with a generic video diffusion-based refinement framework: Match Anything (MAM). 
                With a data collection loop, built the largest internet video correspondence dataset containing over 10K hours of Citywalk videos and travel vlogs. 
                Incorporated 9 highly related tasks into the same framework such as stereo matching, depth, MVS, and video depth. 
                Significantly improved zero-shot performance of 7 downstream applications such as homography estimation, relative pose, visual localization, object pose, SfM, SLAM, and Embodied AI.
              </p>
            </td>
          </tr> -->
          
          <tr onmouseout="fulldit_stop()" onmouseover="fulldit_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='fulldit_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/fulldit/demo.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/fulldit/teaser.png' width=160>
              </div>
              <script type="text/javascript">
                function fulldit_start() {
                  document.getElementById('fulldit_image').style.opacity = "1";
                }

                function fulldit_stop() {
                  document.getElementById('fulldit_image').style.opacity = "0";
                }
                fulldit_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>FullDiT: Multi-Task Video Generative Foundation Model with Full Attention           </papertitle>                </a>
                    <br>
                    <a>Xuan Ju</a>, 
                    <strong>Weicai Ye‚Ä†</strong>, 
                    <a>Quande Liu</a>, 
                    <a>Qiulin Wang</a>,
                    <a>Xintao Wang</a>, 
                    <a>Pengfei Wan</a>, 
                    <a>Di Zhang</a>, 
                    <a>Kun Gai</a>, 
                    <a>Qiang Xu</a> 
                    <br>
                    <em>Arxiv</em> 2025
                    <br>
                    <a href="https://fulldit.github.io/">project page</a>
                    /
                    <a href="">arXiv</a>
                    /
                    <a href="">code</a>
              <p></p>
              <p> A unified video generative foundation model that seamlessly integrates multiple conditions, significantly reduces parameter overhead,
                avoids conflicts common in adapter-based methods, and shows scalability and emergent ability in combining diverse, previously unseen modalities.
                </td> 
          </tr>

          <tr onmouseout="any2caption_stop()" onmouseover="any2caption_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='any2caption_image'><video  width=100% height=100% muted autoplay loop> -->
                <!-- <source src="images/any2caption/demo.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/any2caption/teaser.png' width=160>
              </div>
              <script type="text/javascript">
                function any2caption_start() {
                  document.getElementById('any2caption_image').style.opacity = "1";
                }

                function any2caption_stop() {
                  document.getElementById('any2caption_image').style.opacity = "0";
                }
                any2caption_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  Any2Caption: Interpreting Any Condition to Caption for Controllable Video Generation               </papertitle>                </a>
                    <br>
                    <a>Shengqiong Wu</a>, 
                    <strong>Weicai Ye‚Ä†</strong>, 
                    <a>Jiahao Wang</a>,
                    <a>Quande Liu</a>, 
                    <a>Xintao Wang</a>, 
                    <a>Pengfei Wan</a>, 
                    <a>Di Zhang</a>, 
                    <a>Kun Gai</a>, 
                    <a>Shuicheng Yan</a>,
                    <a>Hao Fei</a>,
                    <a>Tat-Seng Chua</a>
                    <br>
                    <em>Arxiv</em> 2025
                    <br>
                    <a href="https://any2caption.github.io/">project page</a>
                    /
                    <a href="">arXiv</a>
                    /
                    <a href="">code</a>
              <p></p>
              <p>  To address the bottleneck of accurate <strong>user intent interpretation</strong> within current video generation community, present a novel framework that interprets diverse
                 condition into dense, structured captions for controllable video generation from any condition. Significantly improve video quality across various sota video generation models. Propose large-scale high-
                 quality benchmark dataset for any-condition-to-caption task.
                </td> 
          </tr>


          <tr onmouseout="LLaVASLT_stop()" onmouseover="LLaVASLT_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='PhysFlow_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/PhysFlow/PhysFlow.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/LLaVASLT/teaser.png' width=160>
              </div>
              <script type="text/javascript">
                function LLaVASLT_start() {
                  document.getElementById('LLaVASLT_image').style.opacity = "1";
                }

                function LLaVASLT_stop() {
                  document.getElementById('LLaVASLT_image').style.opacity = "0";
                }
                LLaVASLT_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  LLaVA-SLT: Visual Language Tuning for Sign Language Translation
                </papertitle>
              </a>
                    <br>
                    <a>Han Liang</a>, 
                    <a>Chengyu Huang</a>, 
                    <a>Yuecheng Xu</a>, 
                    <a>Cheng Tang</a>, 
                    <strong>Weicai Ye</strong>,
                    <a>Juze Zhang</a>, 
                    <a>Xin Chen</a>, 
                    <a>Jingyi Yu</a>, 
                    <a>Lan Xu</a>
                    <br>
                    <em>Arxiv</em> 2024, Under Review
                    <br>
                    <a href="">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2412.16524">arXiv</a>
                    /
                    <a href="">code</a>
              <p></p>
              <p>Propose scalable MLMM framework tamed for sign language translation. </p>
            </td>
          </tr>

          <tr onmouseout="sketchvideo_stop()" onmouseover="sketchvideo_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='sketchvideo_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/sketchvideo/sketchvideo.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/sketchvideo/teaser.png' width=160>
              </div>
              <script type="text/javascript">
                function sketchvideo_start() {
                  document.getElementById('sketchvideo_image').style.opacity = "1";
                }

                function sketchvideo_stop() {
                  document.getElementById('sketchvideo_image').style.opacity = "0";
                }
                sketchvideo_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  SketchVideo: Sketch-based Video Generation and Editing
                </papertitle>
              </a>
              <br>
                    <a>Feng-Lin Liu</a>, 
                    <a>Hongbo Fu</a>, 
                    <a>Xintao Wang</a>, 
                    <strong>Weicai Ye</strong>,
                    <a>Pengfei Wan</a>, 
                    <a>Di Zhang</a>,
                    <a>Lin Gao</a>
                    <br>
                    <em>CVPR</em> 2025
                    <br>
                    <a href="">project page</a>
                    /
                    <a href="">arXiv</a>
                    /
                    <a href="">code</a>
              <p></p>
              <p>Achieved sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos.
              </p>
            </td>
          </tr>

          <tr onmouseout="PhysFlow_stop()" onmouseover="PhysFlow_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='PhysFlow_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/PhysFlow/PhysFlow.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/PhysFlow/teaser.png' width=160>
              </div>
              <script type="text/javascript">
                function PhysFlow_start() {
                  document.getElementById('PhysFlow_image').style.opacity = "1";
                }

                function PhysFlow_stop() {
                  document.getElementById('PhysFlow_image').style.opacity = "0";
                }
                PhysFlow_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation.
                </papertitle>
              </a>
                    <br>
                    <a>Zhuoman Liu</a>, 
                    <strong>Weicai Ye‚Ä†‚Ä°</strong>,
                    <a>Yan Luximon</a>, 
                    <a>Pengfei Wan</a>, 
                    <a>Di Zhang</a>
                    <br>
                    <em>CVPR</em> 2025
                    <br>
                    <a href="https://zhuomanliu.github.io/PhysFlow/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2411.14423">arXiv</a>
                    /
                    <a href="">code</a>
              <p></p>
              <p>Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation with a differentiable Material Point Method (MPM) and optical flow guidance.
              </p>
            </td>
          </tr>

          <tr onmouseout="Splatter360_stop()" onmouseover="Splatter360_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='PhysFlow_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/Splatter360/Splatter360.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/Splatter360/teaser.png' width=160>
              </div>
              <script type="text/javascript">
                function Splatter360_start() {
                  document.getElementById('Splatter360_image').style.opacity = "1";
                }

                function Splatter360_stop() {
                  document.getElementById('Splatter360_image').style.opacity = "0";
                }
                Splatter360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  Splatter-360: Generalizable 360‚àò Gaussian Splatting for Wide-baseline Panoramic Images
                </papertitle>
              </a>
                    <br>
                    <a>Zheng Chen</a>, 
                    <a>Chenming Wu</a>, 
                    <a>Zhelun Shen</a>, 
                    <a>Chen Zhao</a>, 
                    <strong>Weicai Ye</strong>,
                    <a>Haocheng Feng</a>, 
                    <a>Errui Ding</a>, 
                    <a>Song-Hai Zhang</a>
                    <br>
                    <em>CVPR</em> 2025
                    <br>
                    <a href="https://3d-aigc.github.io/Splatter-360/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2412.06250">arXiv</a>
                    /
                    <a href="https://github.com/thucz/splatter360">code</a>
              <p></p>
              <p>Proposed Generalizable 360‚àò Gaussian Splatting for Wide-baseline Panoramic Images
                </p>
            </td>
          </tr>

          <tr onmouseout="CoSurfGS_stop()" onmouseover="CoSurfGS_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='CoSurfGS_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/FedSurfGS/demo.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/FedSurfGS/framework.jpg' width=160>
              </div>
              <script type="text/javascript">
                function CoSurfGS_start() {
                  document.getElementById('CoSurfGS_image').style.opacity = "1";
                }

                function CoSurfGS_stop() {
                  document.getElementById('CoSurfGS_image').style.opacity = "0";
                }
                CoSurfGS_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  CoSurfGS: Collaborative 3D Surface Gaussian Splatting with Distributed Learning for Large Scene reconstruction               </papertitle>                </a>
                    <br>
                    <a>Yuanyuan Gao</a>, 
                    <a>Yalun Dai</a>, 
                    <a>Hao Li</a>,
                    <strong>Weicai Ye‚Ä†</strong>, 
                    <a>Junyi Chen</a>, 
                    <a>Danpeng Chen</a>, 
                    <a>Dingwen Zhang</a>, 
                    <a>Tong He</a>, 
                    <a>Guofeng Zhang</a> 
                    <a>Junwei Han</a>
                    <br>
                    <em>Arxiv</em> 2024
                    <br>
                    <a href="https://gyy456.github.io/CoSurfGS/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2412.17612">arXiv</a>
                    /
                    <a href="https://github.com/zju3dv/CoSurfGS">code</a>
                    <!-- / -->
                    <!-- <a href="images/PGSR/res3.jpg">Result1</a> -->
                    <!-- / -->
                    <!-- <a href="images/PGSR/res1.jpg">Result2</a> -->
              <p></p>
              <p> First cloud-edge-device hierarchical framework with Distributed Learning for large-scale high-fidelity surface reconstruction, 
                achieving balance between high-precision reconstruction and low-cost memory. 
                </td> 
          </tr>

          <tr onmouseout="gst_stop()" onmouseover="gst_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='gst_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/gst/framework.jpg" type="video/mp4">
                <!-- <img src='images/gst/framework.jpg' width=160> -->
                Your browser does not support the video tag.
                </video></div>
                <img src='images/gst/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function gst_start() {
                  document.getElementById('gst_image').style.opacity = "1";
                }

                function gst_stop() {
                  document.getElementById('gst_image').style.opacity = "0";
                }
                gst_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  Where Am I and What Will I See: An Auto-Regressive Model for Spatial Localization and View Prediction
                </papertitle>
              </a>
                    <br>
                    <a>Junyi Chen</a>, 
                    <a>Di Huang</a>, 
                    <strong>Weicai Ye</strong></a>,
                    <a>Wanli Ouyang</a>,
                    <a>Tong He</a>
                    <br>
                    <em>ICLR</em> 2025
                    <br>
                    <a href="https://sotamak1r.github.io/gst/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2410.18962">arXiv</a>
                    /
                    <a href="https://github.com/SOTAMak1r/GST">code</a>
                    <!-- / -->
                    <!-- <a href="images/gst/framework.jpg">framework</a> -->
              <p></p>
              <p>Proposed a novel auto-regressive framework that jointly addresses spatial localization and view prediction. 
              </p>
            </td>
          </tr>

          <tr onmouseout="meshanything_stop()" onmouseover="meshanything_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='meshanything_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/MeshAnything/demo_video.gif" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/MeshAnything/demo_video.gif' width=160>
              </div>
              <script type="text/javascript">
                function meshanything_start() {
                  document.getElementById('meshanything_image').style.opacity = "1";
                }

                function meshanything_stop() {
                  document.getElementById('meshanything_image').style.opacity = "0";
                }
                meshanything_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers
                  </papertitle>  <br>
                  <a>Yiwen Chen</a>, 
                  <a>Tong He</a>, 
                  <a>Di Huang</a>, 
                  <strong>Weicai Ye</strong>, 
                  <a>Sijin Chen</a>, 
                  <a>Jiaxiang Tang</a>, 
                  <a>Xin Chen</a>,
                  <a>Zhongang Cai</a>, 
                  <a>Lei Yang</a>, 
                  <a>Gang Yu</a>, 
                  <a>Guosheng Lin</a>, 
                  <a>Chi Zhang</a>
                    
                    <br>
                    <em>ICLR</em> 2025
                    <br>
                    <a href="https://buaacyw.github.io/mesh-anything/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2406.10163">arXiv</a>
                    /
                    <a href="https://github.com/buaacyw/MeshAnything">code</a>
                    /
                    <a href="https://huggingface.co/spaces/Yiwen-ntu/MeshAnything">huggingface</a>
              <p></p>
              <p>  MeshAnything mimics human artist in extracting meshes from any 3D representations. 
                It can be combined with various 3D asset production pipelines, such as 3D reconstruction and generation, to convert their results into Artist-Created Meshes that can be seamlessly applied in 3D industry.
                </p>
                              </td> 
          </tr>

          <tr onmouseout="luminat2x_stop()" onmouseover="luminat2x_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pointmamba_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/Lumina-T2X/demo.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/Lumina-T2X/teaser.png' width=160>
              </div>
              <script type="text/javascript">
                function luminat2x_start() {
                  document.getElementById('luminat2x_image').style.opacity = "1";
                }

                function luminat2x_stop() {
                  document.getElementById('luminat2x_image').style.opacity = "0";
                }
                luminat2x_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers
                  </papertitle>                    <br>
                  <a>Peng Gao</a>,
                  <a>Le Zhuo</a>, 
                  <a>Dongyang Liu</a>, 
                  <a>Ruoyi Du</a>, 
                  <a>Xu Luo</a>, 
                  <a>Longtian Qiu</a>, 
                  <a>Yuhang Zhang</a>, 
                  <a>Rongjie Huang</a>, 
                  <a>Shijie Geng</a>, 
                  <a>Renrui Zhang</a>, 
                  <a>unlin Xie</a>, 
                  <a> Wenqi Shao</a>, 
                  <a>Zhengkai Jiang</a>, 
                  <a>Tianshuo Yang</a>, 
                  <strong>Weicai Ye</strong>, 
                  <a>Tong He</a>, <a>Jingwen He</a>, <a>Yu Qiao</a>, <a>Hongsheng Li</a>
                    <br>
                    <em>ICLR</em> 2025, <font color="red">Spotlight</font>, Score: 88866
                    <br>
                    <a href="https://github.com/Alpha-VLLM/Lumina-T2X">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2405.05945">arXiv</a>
                    /
                    <a href="https://github.com/Alpha-VLLM/Lumina-T2X">code</a>
                    / 
                    <a href="http://106.14.2.150:10021/">demo</a>
                    /
                    <a href="https://huggingface.co/Alpha-VLLM/Lumina-T2I">huggingface</a>
              <p></p>
              <p>  Proposed flow-based large diffusion transformers foundation model for transforming text into any modality (image, video, 3D, Audio, music, etc.), resolution, and duration.
                            </p>
                </td> 
          </tr>

          <tr onmouseout="HiSplat_stop()" onmouseover="HiSplat_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='HiSplat_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/HiSplat/HiSplat.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/HiSplat/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function HiSplat_start() {
                  document.getElementById('HiSplat_image').style.opacity = "1";
                }

                function HiSplat_stop() {
                  document.getElementById('HiSplat_image').style.opacity = "0";
                }
                HiSplat_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction
                </papertitle>
              </a>
                    <br>
                    <a>Shengji Tang</a>, 
                    <strong>Weicai Ye‚Ä†</strong></a>,
                    <a>Peng Ye</a>, 
                    <a>Weihao Lin</a>, 
                    <a>Yang Zhou</a>, 
                    <a>Tao Chen</a>, 
                    <a>Wanli Ouyang</a>
                    <br>
                    <em>ICLR</em> 2025
                    <br>
                    <a href="https://open3dvlab.github.io/HiSplat/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2410.06245">arXiv</a>
                    /
                    <a href="https://github.com/Open3DVLab/HiSplat">code</a>
              <p></p>
              <p>Proposed a hierarchical manner in generalizable 3D Gaussian Splatting to construct hierarchical 3D Gaussians via a coarse-to-fine strategy, 
                which significantly enhances reconstruction quality and cross-dataset generalization.
              </p>
            </td>
          </tr>

          <tr onmouseout="ndsdf_stop()" onmouseover="ndf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ndsdf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/ND-SDF/demo.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/ND-SDF/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function ndsdf_start() {
                  document.getElementById('ndsdf_image').style.opacity = "1";
                }

                function ndsdf_stop() {
                  document.getElementById('ndsdf_image').style.opacity = "0";
                }
                ndsdf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction                
                </papertitle>
                </a>
                    <br>
                    <a>Ziyu Tang</a>,
                    <strong>Weicai Ye‚Ä†</strong>,
                    <a>Yifan Wang</a>,
                    <a>Di Huang</a>,
                    <a>Hujun Bao</a>, 
                    <a>Tong He‚Ä†</a>, 
                    <a>Guofeng Zhang</a>
                    <br>
                    <em>ICLR</em> 2025, <font color="red">Spotlight</font>, Score: 8886
                    <br>
                    <a href="https://zju3dv.github.io/nd-sdf/">project page</a>
                    /
                    <a href="">arXiv</a>
                    /
                    <a href="https://github.com/zju3dv/ND-SDF">code</a>
                    /
                    <a href="images/ND-SDF/demo.jpg">More results</a>
              <p></p>
              <p>Proposed Normal Deflection fields to represent the angle deviation between the scene normals and the prior normals, achieving smooth surfaces with fine-grained structures, outperforming MonoSDF.
                </p>
                </td>
                
          </tr>

          <tr onmouseout="gigags_stop()" onmouseover="gigags_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='gigags_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/GigaGS/demo.jpg" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/GigaGS/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function gigags_start() {
                  document.getElementById('gigags_image').style.opacity = "1";
                }

                function gigags_stop() {
                  document.getElementById('gigags_image').style.opacity = "0";
                }
                gigags_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface Reconstruction
                </papertitle>
              </a>
                    <br>
                    <a>Junyi Chen*</a>, 
                    <strong>Weicai Ye*‚Ä†‚Ä°</strong>,
                    <a>Yifan Wang</a>, 
                    <a>Danpeng Chen</a>, 
                    <a>Di Huang</a>, 
                    <a>Wanli Ouyang</a>, 
                    <a>Guofeng Zhang</a>, 
                    <a>Yu Qiao</a>, 
                    <a>Tong He‚Ä†</a>
                    <br>
                    <em>AAAI</em> 2025
                    <br>
                    <a href="https://open3dvlab.github.io/GigaGS/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2409.06685">arXiv</a>
                    /
                    <a href="https://github.com/Open3DVLab/GigaGS">code</a>
                    /
                    <a href="images/GigaGS/demo.jpg">More results</a>
              <p></p>
              <p>Based on PGSR, proposed photorealistic rendering and efficient high-fidelity large surface reconstruction in a divide-and-conquer manner with LOD structure, outperforming Neuralangelo.              </p>
            </td>
          </tr>

          <tr onmouseout="pgsr_stop()" onmouseover="pgsr_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pgsr_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/PGSR/demo.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/PGSR/res2.jpg' width=160>
              </div>
              <script type="text/javascript">
                function pgsr_start() {
                  document.getElementById('pgsr_image').style.opacity = "1";
                }

                function pgsr_stop() {
                  document.getElementById('pgsr_image').style.opacity = "0";
                }
                pgsr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction                </papertitle>                </a>
                    <br>
                    <a>Danpeng Chen</a>, 
                    <a>Hai Li</a>, 
                    <strong>Weicai Ye</strong>, 
                    <a>Yifan Wang</a>, 
                    <a>Weijian Xie</a>, 
                    <a>Shangjin Zhai</a>, 
                    <a>Nan Wang</a>, 
                    <a>Haomin Liu</a>, 
                    <a>Hujun Bao</a>,
                    <a>Guofeng Zhang</a> 
                    <br>
                    <em>TVCG</em> 2024
                    <br>
                    <a href="https://zju3dv.github.io/pgsr/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2406.06521">arXiv</a>
                    /
                    <a href="https://github.com/zju3dv/pgsr">code</a>
                    /
                    <a href="images/PGSR/res3.jpg">Result1</a>
                    /
                    <a href="images/PGSR/res1.jpg">Result2</a>
              <p></p>
              <p> Proposed photorealistic rendering and efficient high-fidelity surface reconstruction model <strong>without any pretrained priors</strong>, outperforming 3DGS-based (Sugar, 2DGS, Gaussian Opacity Fields, etc.) and SDF-Based methods on T&T, DTU, etc. with faster training.
                e.g. (ours: only 1 hour vs Neuralangelo 128+ hours) 
                </td> 
          </tr>

          
          
          <tr onmouseout="diffpano_stop()" onmouseover="diffpano_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='diffpano_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/diffpano/diffpano.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/diffpano/teaser.png' width=160>
              </div>
              <script type="text/javascript">
                function diffpano_start() {
                  document.getElementById('diffpano_image').style.opacity = "1";
                }

                function diffpano_stop() {
                  document.getElementById('diffpano_image').style.opacity = "0";
                }
                diffpano_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion          
                </papertitle>
              </a>
                    <br>
                    <strong>Weicai Ye*‚Ä°</strong> </a>,
                    <a href="">Chenhao Ji*</a>,
                    <a>Zheng Chen</a>,
                    <a href="">Junyao Gao</a>,
                    <a href="">Xiaoshui Huang</a>,
                    <a href="">Song-Hai Zhang</a>,
                    <a>Wanli Ouyang</a>,
                    <a>Tong He‚Ä†</a>,
                    <a>Cairong Zhao‚Ä†</a>,
                    <a href="">Guofeng Zhang‚Ä†</a>
                    <br>
                    <em>NeurIPS</em> 2024
                    <br>
                    <a href="https://zju3dv.github.io/DiffPano/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2410.24203">arXiv</a>
                    /
                    <a href="https://github.com/zju3dv/DiffPano">code</a>
              <p></p>
              <p>Proposed scalable and consistent text-to-panorama generation with spherical epipolar-aware diffusion. 
                Established large-scale panoramic video-text datasets with corresponding depth and camera poses. 
                Achieved long-term, consistent, and diverse panoramic scene generation given unseen text and camera poses with SOTA performance.
              </p>
            </td>
          </tr>
          <tr onmouseout="neurodin_stop()" onmouseover="neurodin_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='neurodin_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/NeuRodin/barn.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/NeuRodin/ballroom.gif' width=160>
              </div>
              <script type="text/javascript">
                function neurodin_start() {
                  document.getElementById('neurodin_image').style.opacity = "1";
                }

                function neurodin_stop() {
                  document.getElementById('neurodin_image').style.opacity = "0";
                }
                neurodin_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface Reconstruction.                    
                </papertitle>
              </a>
    
                    <br>
                    <a>Yifan Wang</a>, 
                    <a>Di Huang</a>, 
                    <strong>Weicai Ye‚Ä†</strong>,
                    <a>Guofeng Zhang</a>, 
                    <a>Wanli Ouyang</a>, 
                    <a>Tong He‚Ä†</a>
                    <br>
                    <em>NeurIPS</em> 2024
                    <br>
                    <a href="https://open3dvlab.github.io/NeuRodin/">project page</a>
                    /
                    <a href="">arXiv</a>
                    /
                    <a href="https://github.com/Open3DVLab/NeuRodin">code</a>
                    / 
                    <a href="images/NeuRodin/cater.gif">result1</a>
                    /
                    <a href="images/NeuRodin/museum.gif">result2</a>
              <p></p>
              <p>Identified two main factors of the SDF-based approach that degrade surface quality and proposed a two-stage neural surface reconstruction framework <strong>without any pretrained priors</strong>, 
                achieving faster training (only 18 GPU hours) and high-fidelity surface reconstruction with fine-grained details, outperforming Neuralangelo on T&T, ScanNet++, etc.
                </p>
                </td>
          </tr>

          <tr onmouseout="pointmatter_stop()" onmouseover="pointmatter_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pointmatter_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/pointmatter/teaser.jpg" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/pointmatter/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function pointmatter_start() {
                  document.getElementById('pointmatter_image').style.opacity = "1";
                }

                function pointmatter_stop() {
                  document.getElementById('pointmatter_image').style.opacity = "0";
                }
                pointmatter_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning
                </papertitle>                </a>
                    <br>
                    <a>Haoyi Zhu</a>, 
                    <a>Yating Wang</a>, 
                    <a>Di Huang</a>, 
                    <strong>Weicai Ye</strong>, 
                    <a>Wanli Ouyang</a>, 
                    <a>Tong He</a>
                    
                    
                    <br>
                    <em>NeurIPS 2024 Datasets and Benchmarks Track</em> 
                    <br>
                    <a href="https://haoyizhu.github.io/PointCloudMatters.github.io/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2402.02500">arXiv</a>
                    /
                    <a href="https://github.com/HaoyiZhu/PointCloudMatters">code</a>
              <p></p>
              <p>  Implied that point cloud observation, or explicit 3D information, matters for robot learning. With point cloud as input, the agent achieved higher mean success rates and exhibited better generalization ability.</p>

                </td> 
          </tr>


          <tr onmouseout="DGTR_stop()" onmouseover="DGTR_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='DGTR_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/DGTR/DGTR.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/DGTR/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function DGTR_start() {
                  document.getElementById('DGTR_image').style.opacity = "1";
                }

                function DGTR_stop() {
                  document.getElementById('DGTR_image').style.opacity = "0";
                }
                DGTR_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  DGTR: Distributed Gaussian Turbo-Reconstruction for Sparse-View Vast Scenes
                </papertitle>
              </a>
                    <br>
                    <a>Hao Li</a>, 
                    <a>Yuanyuan Gao</a>, 
                    <a>Haosong Peng</a>, 
                    <a>Chenming Wu</a>, 
                    <strong>Weicai Ye</strong>,
                    <a>Yufeng Zhan</a>,
                    <a>Chen Zhao</a>, 
                    <a>Dingwen Zhang</a>, 
                    <a>Jingdong Wang</a>, 
                    <a>Junwei Han</a>
                    <br>
                    <em>ICRA</em> 2025
                    <br>
                    <a href="https://3d-aigc.github.io/DGTR/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2411.12309">arXiv</a>
                    /
                    <a href="https://3d-aigc.github.io/DGTR/">code</a>
                    <!-- / -->
                    <!-- <a href="images/DGTR/framework.jpg">framework</a> -->
              <p></p>
              <p>
                Proposed a novel distributed framework for efficient
                Gaussian reconstruction for sparse-view vast scenes, leveraging feed-forward
                Gaussian model for fast inference and a global alignment algorithm to ensure geometric
                consistency.
              </p>
            </td>
          </tr>

          
          

          
          

          

          <tr onmouseout="DynaSurfGS_stop()" onmouseover="DynaSurfGS_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='DynaSurfGS_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/DynaSurfGS/teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/DynaSurfGS/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function DynaSurfGS_start() {
                  document.getElementById('DynaSurfGS_image').style.opacity = "1";
                }

                function DynaSurfGS_stop() {
                  document.getElementById('DynaSurfGS_image').style.opacity = "0";
                }
                DynaSurfGS_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  DynaSurfGS: Dynamic Surface Reconstruction with Planar-based Gaussian Splatting
                </papertitle>
              </a>
                    <br>
                    <a>Weiwei Cai</a>, 
                    <strong>Weicai Ye‚Ä†‚Ä°</strong>,
                    <a>Peng Ye</a>, 
                    <a>Tong He</a>, 
                    <a>Tao Chen‚Ä†</a>
                    <br>
                    <em>Arxiv</em> 2024, Under Review
                    <br>
                    <a href="https://open3dvlab.github.io/DynaSurfGS">project page</a>
                    /
                    <a href="">arXiv</a>
                    /
                    <a href="https://github.com/Open3DVLab/DynaSurfGS">code</a>
              <p></p>
              <p>
                Based on PGSR, proposed the DynaSurfGS framework, which can facilitate real-time photorealistic rendering and dynamic high-fidelity surface reconstruction, 
                achieving smooth surfaces with meticulous geometry.
              </p>
            </td>
          </tr>

          
          
          <tr onmouseout="streetsurfgs_stop()" onmouseover="streetsurfgs_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='streetsurfgs_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/StreetSurfGS/teaser.jpg" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/StreetSurfGS/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function streetsurfgs_start() {
                  document.getElementById('streetsurfgs_image').style.opacity = "1";
                }

                function streetsurfgs_stop() {
                  document.getElementById('streetsurfgs_image').style.opacity = "0";
                }
                streetsurfgs_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  StreetSurfGS: Scalable Large Scene Surface Reconstruction with Gaussian Splatting for Urban Street Scences                   
                </papertitle>
                </a>
                    <br>
                    <a>Xiao Cui*</a>
                    <strong>Weicai Ye*‚Ä†‚Ä°</strong>,
                    <a>Yifan Wang</a>, 
                    <a>Guofeng Zhang</a>, 
                    <a>Wengang Zhou</a>, 
                    <a>Tong He‚Ä†</a>, 
                    <a>Houqiang Li</a>
                    <br>
                    <em>TCSVT</em> 2025
                    <br>
                    <a href="">project page</a>
                    /
                    <a href="">arXiv</a>
                    /
                    <a href="">code</a>
              <p></p>
              <p>Based on PGSR, proposed photorealistic rendering and efficient high-fidelity Large Scene Surface Reconstruction for Urban Street Scenes with Free Camera Trajectories, outperforming F2NeRF.
                </p>
              </td>
          </tr>

          <tr onmouseout="datapsfm_stop()" onmouseover="datapsfm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='datapsfm_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/DATAP-SfM/framework.jpg" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/DATAP-SfM/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function datapsfm_start() {
                  document.getElementById('datapsfm_image').style.opacity = "1";
                }

                function datapsfm_stop() {
                  document.getElementById('datapsfm_image').style.opacity = "0";
                }
                datapsfm_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  DATAP-SfM: Dynamic-Aware Tracking Any Point for Robust Dense Structure from Motion in the Wild              
                </papertitle>
              </a>
                    <br>
                    <strong>Weicai Ye‚Ä°</strong>,
                    <a>Xinyu Chen</a>, 
                    <a>Ruohao Zhan</a>, 
                    <a>Di Huang</a>, 
                    <a>Xiaoshui Huang</a>, 
                    <a>Haoyi Zhu</a>, 
                    <a>Hujun Bao</a>, 
                    <a>Wanli Ouyang</a>, 
                    <a>Tong He‚Ä†</a>,
                    <a href="">Guofeng Zhang‚Ä†</a>
                    <br>
                    <em>Arxiv</em> 2024
                    <br>
                    <a href="">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2411.13291">arXiv</a>
                    /
                    <a href="https://github.com/zju3dv/DATAP-SfM">code</a>
              <p></p>
              <p>Proposed a concise, elegant, and robust SfM pipeline with point tracking for smooth camera trajectories and dense pointclouds from casual monocular videos.        
              </p>
            </td>
          </tr>
          
          
          <tr onmouseout="d3flowslam_stop()" onmouseover="d3flowslam_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='d3flowslam_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/d3flowslam/framework.jpg" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/D3FlowSLAM/framework.jpg' width=160>
              </div>
              <script type="text/javascript">
                function d3flowslam_start() {
                  document.getElementById('d3flowslam_image').style.opacity = "1";
                }

                function d3flowslam_stop() {
                  document.getElementById('d3flowslam_image').style.opacity = "0";
                }
                d3flowslam_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  D3FlowSLAM: Self-Supervised Dynamic SLAM with Flow Motion Decomposition and DINO Guidance              
                  </papertitle>
                </a>
                    <br>
                    <a>Xingyuan Yu*</a>,
                    <strong>Weicai Ye*‚Ä°</strong>,
                    <a>Xiyue Guo</a>,
                    <a>Yuhang Ming</a>,
                    <a>Jinyu Li</a>,
                    <a>Hujun Bao</a>, 
                    <a>Zhaopeng Cui</a>, 
                    <a>Guofeng Zhang‚Ä†</a>
                    <br>
                    <em>Arxiv</em> 2024, Under Review
                    <br>
                    <a href="">project page</a>
                    /
                    <a href="">arXiv</a>
                    /
                    <a href="">code</a>
              <p></p>
              <p>  Proposed self-supervised dynamic SLAM with Flow Motion Decomposition and DINO Guidance, outperforming DROID-SLAM.
                                </p>
                </td> 
          </tr>

          <tr onmouseout="VIPeR_stop()" onmouseover="VIPeR_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='pointmamba_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/VIPeR/" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/VIPeR/teaser.png' width=160>
              </div>
              <script type="text/javascript">
                function VIPeR_start() {
                  document.getElementById('VIPeR_image').style.opacity = "1";
                }

                function VIPeR_stop() {
                  document.getElementById('VIPeR_image').style.opacity = "0";
                }
                VIPeR_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  VIPeR: Visual Incremental Place Recognition with Adaptive Mining and Continual Learning </papertitle>                </a>
                    <br>
                    <a>Yuhang Ming</a>, 
                    <a>Minyang Xu</a>, 
                    <a>Xingrui Yang</a>, 
                    <strong>Weicai Ye</strong>, 
                    <a>Weihan Wang</a>, 
                    <a>Yong Peng</a>, 
                    <a>Weichen Dai</a>, 
                    <a>Wanzeng Kong</a>
                    <br>
                    <em>RA-L</em> 2025
                    <br>
                    <a href="">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2407.21416">arXiv</a>
                    /
                    <a href="">code</a>
              <p></p>
              <p>  Proposed a visual incremental place recognition method with adaptive mining and lifelong learning
                </p>
                </td> 
          </tr>

          
         
          <tr onmouseout="pointmamba_stop()" onmouseover="pointmamba_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pointmamba_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/PointMamba/" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/PointMamba/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function pointmamba_start() {
                  document.getElementById('pointmamba_image').style.opacity = "1";
                }

                function pointmamba_stop() {
                  document.getElementById('pointmamba_image').style.opacity = "0";
                }
                pointmamba_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  Point Mamba: A Novel Point Cloud Backbone Based on State Space Model with Octree-Based Ordering Strategy.                </papertitle>                </a>
                    <br>
                    <a>Jiuming Liu</a>, 
                    <a>Ruiji Yu</a>, 
                    <a>Yian Wang</a>, 
                    <a>Yu Zheng</a>, 
                    <a>Tianchen Deng</a>, 
                    <strong>Weicai Ye</strong>, 
                    <a>Hesheng Wang</a>
                    <br>
                    <em>Arxiv</em> 2024, Under Review
                    <br>
                    <a href="">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2403.06467">arXiv</a>
                    /
                    <a href="https://github.com/IRMVLab/Point-Mamba">code</a>
              <p></p>
              <p>  Proposed efficient point cloud backbone with Mamba framework and achieved SOTA performance.
                </p>
                </td> 
          </tr>

          

          <tr onmouseout="nerfdetpp_stop()" onmouseover="nerfdetpp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfdetpp_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/NeRFDet++/video_small.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/NeRFDet++/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function nerfdetpp_start() {
                  document.getElementById('nerfdetpp_image').style.opacity = "1";
                }

                function nerfdetpp_stop() {
                  document.getElementById('nerfdetpp_image').style.opacity = "0";
                }
                nerfdetpp_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>
                  NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection 
                  </papertitle>                   <br>
                  <a>Chenxi Huang</a>, 
                  <a>Yuenan Hou</a>, 
                  <strong>Weicai Ye</strong>, 
                  <a>Di Huang</a>, 
                  <a>Xiaoshui Huang</a>, 
                  <a>Binbin Lin</a>, 
                  <a>Deng Cai</a>, 
                  <a>Wanli Ouyang</a>
                    <br>
                    <em>Arxiv</em> 2024, Under Review
                    <br>
                    <a href="">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2402.14464">arXiv</a>
                    /
                    <a href="https://github.com/mrsempress/NeRF-Detplusplus">code</a>
                    /
                    <a href="images/NeRFDet++/presentation.mp4">Video</a>
                    
              <p></p>
              <p> Incorporating semantic cues and perspective-aware depth supervision, NeRF-Det++ outperforms NeRF-Det by +1.9% in mAP@0.25 and +3.5% in mAP@0.50 on ScanNetV2.
                </p>
                              </td> 
          </tr>

          <tr onmouseout="difflow3d_stop()" onmouseover="difflow3d_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='difflow3d_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/DifFlow3D/teaser.jpg" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/DifFlow3D/teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function difflow3d_start() {
                  document.getElementById('difflow3d_image').style.opacity = "1";
                }

                function difflow3d_stop() {
                  document.getElementById('difflow3d_image').style.opacity = "0";
                }
                difflow3d_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Diffusion Model  </papertitle>
                    </a>
                    <br>
                    <a>Jiuming Liu</a>, <a>Guangming Wang</a>, 
                    <strong>Weicai Ye</strong>,
                    <a>Chaokang Jiang</a>, 
                    <a>Jinru Han</a>, 
                    <a>Zhe Liu</a>, <a>Guofeng Zhang</a>, 
                    <a>Dalong Du</a>, 
                    <a>Hesheng Wang</a>
                    
                    <br>
                    <em>CVPR</em> 2024
                    <br>
                    <a href="https://arxiv.org/abs/2311.17456">arXiv</a>
                    /
                    <a href="https://github.com/IRMVLab/DifFlow3D">code</a>
                    /
                    <a href="images/DifFlow3D/demo.mp4">demo</a>
              <p></p>
              <p>
                Proposed plug-and-play and iterative diffusion refinement framework for robust scene flow estimation. Achieved unprecedented millimeter level accuracy on KITTI, and with 6.7% and 19.1% EPE3D reduction respectively on FlyingThings3D and KITTI 2015.              </p>
            </td>
          </tr>

          <tr onmouseout="intrinsicnerf_stop()" onmouseover="intrinsicnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='intrinsicnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/intrinsicnerf/teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/intrinsicnerf/teaser.jpeg' width=160>
              </div>
              <script type="text/javascript">
                function intrinsicnerf_start() {
                  document.getElementById('intrinsicnerf_image').style.opacity = "1";
                }

                function intrinsicnerf_stop() {
                  document.getElementById('intrinsicnerf_image').style.opacity = "0";
                }
                intrinsicnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10377702/">
                <papertitle>IntrinsicNeRF:
                  Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis
                </papertitle>
                    </a>
                    <br>
                    <strong>Weicai Ye*</strong>,
                    <a href="">Shuo Chen*</a>,
                    <a>Chong Bao</a>,
                    <a href="">Hujun Bao</a>,
                    <a href="">Marc Pollefeys</a>,
                    <a href="">Zhaopeng Cui</a>,
                    <a href="">Guofeng Zhang</a>
                  
                    <br>
                    <em>ICCV</em> 2023
                    <br>
                    <a href="https://zju3dv.github.io/intrinsic_nerf/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2210.00647">arXiv</a>
                    /
                    <a href="https://github.com/zju3dv/IntrinsicNeRF">code</a>
                    /
                    <a href="images/intrinsicnerf/IntrinsicNeRF_poster.pdf">poster</a>
              <p></p>
              <p>Introduced intrinsic decomposition into the NeRF-based rendering and performed editable novel view synthesis in room-scale scenes.
              </p>
            </td>
          </tr>

          <tr onmouseout="pvo_stop()" onmouseover="pvo_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pvo_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/PVO/pvo_teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/PVO/pvo_teaser.jpg' width=160>
              </div>
              <script type="text/javascript">
                function pvo_start() {
                  document.getElementById('pvo_image').style.opacity = "1";
                }

                function pvo_stop() {
                  document.getElementById('pvo_image').style.opacity = "0";
                }
                pvo_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10204808">
                <papertitle>PVO: Panoptic Visual Odometry  </papertitle>
                    </a>
                    <br>
                    <strong>Weicai Ye*</strong>,
                    <a href="">Xinyue Lan*</a>,
                    <a>Shuo Chen</a>,
                    <a>Yuhang Ming</a>,
                    <a>Xingyuan Yu</a>,
                    <a>Zhaopeng Cui</a>,
                    <a href="">Hujun Bao</a>,
                    <a href="">Guofeng Zhang</a>
                    <br>
                    <em>CVPR</em> 2023
                    <br>
                    <a href="https://zju3dv.github.io/pvo/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2207.01610">arXiv</a>
                    /
                    <a href="https://github.com/zju3dv/pvo">code</a>
                    /
                    <a href="images/PVO/PVO_poster.pdf">poster</a>
              <p></p>
              <p>
                Introduced panoptic visual odometry framework to achieve comprehensive modeling of the scene motion, geometry, and panoptic segmentation information.
              </p>
            </td>
          </tr>

          <tr onmouseout="deflowslam_stop()" onmouseover="deflowslam_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px"> -->
              <div class="one">
                <div class="two" id='bgnet_image'>
                  <img src='images/DeFlowSLAM/deflowslam_teaser.jpg' width="160">
                </div>
                <img src='images/DeFlowSLAM/deflowslam_teaser.jpg' width="160">
              </div>
              <script type="text/javascript">
                function deflowslam_start() {
                  document.getElementById('deflowslam_image').style.opacity = "1";
                }

                function deflowslam_stop() {
                  document.getElementById('deflowslam_image').style.opacity = "0";
                }
                deflowslam_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9284705">
                <papertitle>DeFlowSLAM: Self-Supervised Scene Motion Decomposition for Dynamic Dense SLAM
                </papertitle>
                </a>
              <br>
              <strong>Weicai Ye*</strong>,
              <a>Xingyuan Yu*</a>, 
              <a>Xinyue Lan</a>, 
              <a>Yuhang Ming</a>, 
              <a>Jinyu Li</a>,
              <a>Zhaopeng Cui</a>,
              <a href="">Hujun Bao</a>,
              <a href="">Guofeng Zhang</a>
              <br>
              <em>Arxiv</em> 2022
              <br>
              <a href="https://zju3dv.github.io/deflowslam/">Project</a>
              /
              <a href="https://github.com/zju3dv/DeFlowSLAM">code</a>
              /
              <a href="https://arxiv.org/abs/2207.08794">arxiv</a>
              <p></p>
              <p>
                Proposed a novel dual-flow representation of selfsupervised scene motion decomposition for dynamic dense SLAM
              </p>
                       </td>
          </tr>	

          <tr onmouseout="idfslam_stop()" onmouseover="idfslam_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='idfslam_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/iDF-SLAM/overview.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/iDF-SLAM/idfslam.gif' width=160>
              </div>
              <script type="text/javascript">
                function idfslam_start() {
                  document.getElementById('idfslam_image').style.opacity = "1";
                }

                function idfslam_stop() {
                  document.getElementById('idfslam_image').style.opacity = "0";
                }
                idfslam_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>iDF-SLAM: End-to-End RGB-D SLAM with Neural Implicit Mapping and Deep Feature Tracking
                  </papertitle>
                </a>
                    <br>
                    <a>Yuhang Ming</a>,
                    <strong>Weicai Ye</strong>,
                    <a href="">Andrew Calway</a>
                    <br>
                    <em>Arxiv</em> 2022
                    <br>
                    <a href="https://arxiv.org/abs/2209.07919">arXiv</a>
                    /
                    <a href="images/iDF-SLAM/iDF-SLAM.mp4">video</a>
              <p></p>
              <p>Proposed a novel end-to-end RGB-D SLAM, which adopts a feature-based deep neural tracker as frontend and a NeRF-based neural implicit mapper as the backend.                </p>
              </td>
          </tr>

          <tr onmouseout="gam_stop()" onmouseover="gam_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">           
                  <div class="two" id='gam_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/GAM/GAM.gif" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/GAM/GAM.gif' width=160>
                </div>
              <script type="text/javascript">
                function gam_start() {
                  document.getElementById('gam_image').style.opacity = "1";
                }

                function gam_stop() {
                  document.getElementById('gam_image').style.opacity = "0";
                }
                gam_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Improving Feature-based Visual Localization by Geometry-Aided Matching
                </papertitle>
                </a>
              <br>
              <a href="">Hailin Yu</a>, 
              <a href="">Youji Feng</a>,
              <strong>Weicai Ye</strong>,
              <a >Mingxuan Jiang</a>,
              <a href="">Hujun Bao</a>,
              <a href="">Guofeng Zhang</a>
              <br>
              <em>Arxiv</em> 2022
              <br>
              <a href="https://arxiv.org/abs/2211.08712">arxiv</a>
              /
              <a href="https://github.com/openxrlab/xrlocalization">code</a>
              /
              <a href="images/GAM/GAM.gif">video</a>
              /
              <p></p>
              <p>
                As a main solution for feature matching and visual localization, integrated into OpenXRLab.           </td>
          </tr>	

          <tr onmouseout="hybridtracker_stop()" onmouseover="hybridtracker_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hybridtracker_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/HybridTracker/hybridtracker.gif" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/HybridTracker/hybridtracker.gif' width="160">
              </div>
              <script type="text/javascript">
                function hybridtracker_start() {
                  document.getElementById('hybridtracker_image').style.opacity = "1";
                }

                function hybridtracker_stop() {
                  document.getElementById('hybridtracker_image').style.opacity = "0";
                }
                hybridtracker_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Hybrid Tracker with Pixel and Instance for Video Panoptic Segmentation
                </papertitle>
                </a>
              <br>
              <strong>Weicai Ye*</strong>,
              <a>Xinyue Lan*</a>, 
              <a>Ge Su</a>,
              <a>Zhaopeng Cui</a>,
              <a href="">Hujun Bao</a>,
              <a href="">Guofeng Zhang</a>
              <br>
              <em>Arxiv</em> 2022
              <br>
              <a href="https://arxiv.org/abs/2203.01217">arxiv</a>
              /
              <a href="images/HybridTracker/hybridtracker.mp4">video</a>
              <p></p>
              <p>
                Achieved SOTA performance on video panoptic segmentation from two perspectives: feature space (Instance Tracker) and spatial location (Pixel Tracker).              </p>
                       </td>
          </tr>	

          <tr onmouseout="coxgraph_stop()" onmouseover="coxgraph_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='coxgraph_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/Coxgraph/IROS2021_Coxgraph.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/Coxgraph/IROS2021_Coxgraph.png' width=160>
              </div>
              <script type="text/javascript">
                function coxgraph_start() {
                  document.getElementById('coxgraph_image').style.opacity = "1";
                }

                function coxgraph_stop() {
                  document.getElementById('coxgraph_image').style.opacity = "0";
                }
                coxgraph_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9636645/">
                <papertitle>Coxgraph: Multi-Robot Collaborative, Globally Consistent, Online Dense Reconstruction System                </papertitle>
                    </a>
                    <br>
                    <a>Xiangyu Liu</a>,
                    <strong>Weicai Ye</strong>,
                    <a href="">Chaoran Tian</a>,
                    <a href="">Zhaopeng Cui</a>,
                    <a href="">Hujun Bao</a>,
                    <a href="">Guofeng Zhang</a>
                  
                    <br>
                    <em>IROS</em> 2021, <font color="red">Best Paper Award Finalist on Safety, Security, and Rescue Robotics in memory of Motohiro Kisoi.</font>
                    <br>
                    <a href="https://github.com/zju3dv/coxgraph/">project page</a>
                    /
                    <a href="images/Coxgraph/IROS2021_Coxgraph.pdf">arXiv</a>
                    /
                    <a href="https://github.com/zju3dv/coxgraph">code</a>
                    /
                    <a href="images/Coxgraph/IROS2021_Coxgraph.mp4">video</a>
              <p></p>
              <p>Proposed an efficient system named Coxgraph for multi-robot collaborative dense reconstruction in real-time. 
                To facilitate transmission, we propose a compact 3D representation which transforms the SDF submap to mesh packs.
                </p>
              </td>
          </tr>

          <tr onmouseout="arcargo_stop()" onmouseover="arcargo_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px"> -->
              <div class="one">
                <div class="two" id='arcargo_image'>
                  <img src='images/arcargo/teaser_1.png' width="160">
                </div>
                <img src='images/arcargo/teaser_2.png' width="160">
              </div>
              <script type="text/javascript">
                function arcargo_start() {
                  document.getElementById('arcargo_image').style.opacity = "1";
                }

                function arcargo_stop() {
                  document.getElementById('arcargo_image').style.opacity = "0";
                }
                arcargo_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9730267">
                <papertitle>ARCargo: Multi-Device Integrated Cargo Loading Management System with Augmented Reality</papertitle>
              </a>
              <br>
              <a href="">Tianxiang Zhang</a>, 
              <a href="">Chong Bao</a>,
              <a href="">Hongjia Zhai</a>,
              <a href="">Jiazhen Xia</a>,
              <strong>Weicai Ye‚Ä°</strong>,
              <a href="">Guofeng Zhang</a>
              <br>
              <em>CyberSciTech</em> 2021
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9730267">paper</a>
              /
              <a href="https://www.youtube.com/watch?v=La7TNMIDWvY">video</a>
              <p></p>
              <p>
                Proposed a multi-device integrated cargo loading management system with AR, which monitors cargoes by fusing perceptual information from multiple devices in real-time.              </p>
            </td>
          </tr>	

          <tr onmouseout="superplane_stop()" onmouseover="superplane_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='superplane_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/SuperPlane/VR2021_SuperPlane.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/SuperPlane/VR2021_SuperPlane.png' width=160>
              </div>
              <script type="text/javascript">
                function superplane_start() {
                  document.getElementById('superplane_image').style.opacity = "1";
                }

                function superplane_stop() {
                  document.getElementById('superplane_image').style.opacity = "0";
                }
                superplane_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9417783/">
                <papertitle>SuperPlane: 3D Plane Detection and Description from a Single Image
                </papertitle>
                    </a>
                    <br>
                    <strong>Weicai Ye</strong>,
                    <a href="">Hai Li</a>,
                    <a>Tianxiang Zhang</a>,
                    <a href="">Xiaowei Zhou</a>,
                    <a href="">Hujun Bao</a>,
                    <a href="">Guofeng Zhang</a>
                    <br>
                    <em>VR</em> 2021
                    <br>
                    <a href="images/SuperPlane/VR2021_SuperPlane.pdf">paper</a>
                    /
                    <a href="https://youtu.be/cZW1YHuF-rM">video</a>
              <p></p>
              <p>
                Introduced robust plane matching in texture-less scenes and achieved SOTA performance in image-based localization.              </p>
            </td>
          </tr>          

          <tr onmouseout="saliency_stop()" onmouseover="saliency_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='saliency_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/Saliency/3DV2020_Saliency.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/Saliency/3DV2020_Saliency.png' width=160>
              </div>
              <script type="text/javascript">
                function saliency_start() {
                  document.getElementById('saliency_image').style.opacity = "1";
                }

                function saliency_stop() {
                  document.getElementById('saliency_image').style.opacity = "0";
                }
                saliency_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9320412/">
                <papertitle>Saliency Guided Subdivision for Single-View Mesh Reconstruction
                </papertitle>
                    </a>
                    <br>
                    <a href="">Hai Li*</a>,
                    <strong>Weicai Ye*</strong>,
                    <a>Guofeng Zhang</a>, 
                    <a>Sanyuan Zhang</a>,
                    <a>Hujun Bao</a>
                    <br>
                    <em>3DV</em> 2020
                    <br>
                    <a href="images/SuperPlane/VR2021_SuperPlane.pdf">paper</a>
                    /
                    <a href="https://www.youtube.com/watch?v=fgxplMU0H3M">video</a>
                    /
                    <a href="images/Saliency/3DV2020_Saliency_poster.pdf">poster</a>
                    /
                    <a href="images/Saliency/3DV2020_Saliency_slides.pptx">slides</a>
              <p></p>
              <p>
                Proposed a novel saliency guided subdivision method to achieve the trade-off between detail generation and memory consumption. Our method can both produce visually pleasing mesh reconstruction results with fine details and achieve better performance.
                </p>
            </td>
          </tr>

          <tr onmouseout="bgnet_stop()" onmouseover="bgnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px"> -->
              <div class="one">
                <div class="two" id='bgnet_image'>
                  <img src='images/BGNet/ISMAR2020_BGNet.png' width="160">
                </div>
                <img src='images/BGNet/ISMAR2020_BGNet.png' width="160">
              </div>
              <script type="text/javascript">
                function bgnet_start() {
                  document.getElementById('bgnet_image').style.opacity = "1";
                }

                function bgnet_stop() {
                  document.getElementById('bgnet_image').style.opacity = "0";
                }
                bgnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9284705">
                <papertitle>Learning Bipartite Graph Matching for Camera Localization
                  </papertitle>
                </a>
              <br>
              <a href="">Hailin Yu</a>, 
              <strong>Weicai Ye</strong>,
              <a href="">Youji Feng</a>,
              <a href="">Hujun Bao</a>,
              <a href="">Guofeng Zhang</a>
              <br>
              <em>ISMAR</em> 2020
              <br>
              <a href="images/BGNet/ISMAR2020_BGNet.pdf">paper</a>
              /
              <a href="https://docs.google.com/presentation/d/1fEzsb3RmZIa8kfDGopNcZ62agY5FuTeQJu3xj9XjNrM/mobilepresent?slide=id.ga6f4ee1874_0_1">poster</a>
              <p></p>
              <p>
                Proposed bipartite graph network with Hungarian pooling layer to deal with 2D-3D matching, which can find more correct matches and improves localization on both the robustness and accuracy.
           </td>
          </tr>	       
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Talks</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sensetime_lego.png"></td> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="talks/valse20250312.png" alt="valse20250312" width="160"></td>
            <td width="75%" valign="center">
              Multi-modal Video Generative Foundation models
              <br><a href="talks/multi-modal video generative foundation models.pdf">Slides</a>
              <br> 
              <a href="https://mp.weixin.qq.com/s/aq2Z-kq-z-x2tYWEc0URdA">Valse Webinar</a>, 2025.03.12
            </td>
          </tr>

         
          
        </tbody></table>
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Experiences</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sensetime_lego.png"></td> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/shailab.jpg" alt="shailab" width="160"></td>
            <td width="75%" valign="center">
              <!-- <a >[2020.07-2020.09]  Computer Vision Algorithm Intern, Multi-sensor Fusion Localization Group, Sensetime</a> -->
              Researcher Intern
              <br><a href="https://github.com/Open3DVLab">General 3D Vision Team</a>, <a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a>
              <br> 
              As the first author/corresponding author/project lead, proposed InternVerse (Reconstruction Foundation Models include CoSurfGS, GigaGS, StreetSurfGS, NeuRodin), 
              DiffPano (Text to Multi-view Panorama Generation), etc. 
              Working with <a href="https://tonghe90.github.io/">Dr. Tong He</a>, <a href="https://wlouyang.github.io/">Prof. Wanli Ouyang</a>, and <a href="https://mmlab.siat.ac.cn/yuqiao">Prof. Yu Qiao</a>.
              Mentoring 10+ junior researchers at Shanghai AI Lab.
              <br>
              2023.10-2024.09
            </td>
          </tr>

          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sensetime_lego.png"></td> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ETHZ.png" alt="ethz" width="160"></td>
            <td width="75%" valign="center">
              <!-- <a >[2020.07-2020.09]  Computer Vision Algorithm Intern, Multi-sensor Fusion Localization Group, Sensetime</a> -->
              Visiting Researcher
              <br><a href="https://cvg.ethz.ch/">Computer Vision and Geometry Lab</a>, <a href="https://ethz.ch/en.html">ETH Z√ºrich</a>,
              advised by <a href="https://people.inf.ethz.ch/marc.pollefeys/"> Prof. Marc Pollefeys </a>
              <br>
              2022.09-2023.03
            </td>
          </tr>

          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sensetime_lego.png"></td> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sensetime.png" alt="sensetime" width="160"></td>
            <td width="75%" valign="center">
              <!-- <a >[2020.07-2020.09]  Computer Vision Algorithm Intern, Multi-sensor Fusion Localization Group, Sensetime</a> -->
              3D Vision Researcher Intern
              <br>3D Reconstruction of Indoor Scene of RGB-D Images, <a href="https://www.sensetime.com/">Sensetime</a>
              <br> 2018.01-2018.05
            </td>
          </tr>
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sensetime_lego.png"></td> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/baidu.png" alt="baidu" width="160"></td>
            <td width="75%" valign="center">
              Software Engineer Intern 
              <br> Video Search System, <a href="https://www.baidu.com/">Baidu</a>
              <br> 2017.02-2017.07
              <br>
            </td>
          </tr>
        </tbody></table>

        

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Adwards and Honors</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <ul>
            <li>2022 <strong>Academic Rising Star at Zhejiang University.</strong></li>
            <li>2021 <a href="https://www.iros2021.org/awards" target="_blank" rel="noopener">Best Paper Award Finalist on Safety, Security, and Rescue Robotics in memory of Motohiro Kisoi (IROS2021).</a></li>
            <li>2020 <strong>5th</strong> of ECCV GigaVision Challenge.</li>
            <li>2020 <strong>6th</strong> among 1945 teams in the Taobao Live Product Identification Contest.</li>
            <li>2019 <strong>Zhijun He Outstanding Scholarship</strong>.</li>
            <li>2019 <strong>Chiang Chen Industrial Charity Foundation Grant</strong>.</li>
            <li>2018 <strong>Champion</strong> of 2018 Cloudwalk Headcount Challenge with 31,500¬• Bonus.</li>
            <li>2017 <strong>National Encouragement Scholarship</strong>, Ranked 3rd of the 111 Students.</li>
            <li>2017 <strong>Meritorious Winner</strong> in Mathematical Contest Modeling.</li>
            <li>2016 <strong>First Prize</strong> in Sichuan Province Contest District in China Undergraduate Mathematical Contest in Modeling.</li>
          </ul>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <ul>
            <li>Conference Reviewer: ICML, NeurIPS, ICLR, CVPR, ECCV, ICCV, ICRA, IROS, VR, ISMAR, AAAI</li>
            <li>Journal Reviewer: TIP, TCSVT, RAL, CVM</li>
          </ul>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Collaborators</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <ul>
            Mentored 20+ Junior Researchers at Shanghai AI Lab, ETH, TUM, CUHK, NTU, THU, ZJU, SJTU, FDU, USTC, NWPU, and TJU.            
          </ul>        
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XJM8LRPHMK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-XJM8LRPHMK');
</script>
</body>

</html>
