<html>

<head>
<title>Weicai Ye's Homepage</title>
<style type="text/css" media="screen">
html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p,
blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em,
font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt,
dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot,
thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration: none;
}

a:focus,
a:hover {
  color: #f09228;
  text-decoration: none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h4 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
}

ul {
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

strong,
b {
  font-weight: bold;
}

em,
i {
  font-style: italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.3em;
  margin-bottom: 0.3em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-right: 230px;
}

div.paper div.wide {
  padding-right: 0px;
}

img.paper {
  margin-bottom: 0.5em;
  float: right;
  width: 200px;
}

span.blurb {
  font-style: italic;
  display: block;
  margin-top: 0.75em;
  margin-bottom: 0.5em;
}

span.news {
  display: block;
  margin-top: 0.5em;
}

pre,
code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-111088147-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

<script type="text/javascript" src="js/hidebib.js"></script>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
</head>

<body>

<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
  <div style="margin: 0px auto; width: 100%">
    <img title="Weicai Ye (叶伟才) " style="float: left; padding-left: 4em; height: 130px;" src="yeweicai.jpg">
    <div style="padding-left: 20em; vertical-align: top; height: 100px;">
      <span style="line-height: 300%; font-size: 16pt;">Weicai Ye 叶伟才 </span><br />
      <span>Ph.D. Candidate</span><br />
      <span>3D Vision Group, State Key Lab of CAD&CG</span><br />
      <span>College of Computer Science, Zhejiang University</span><br />
      <!-- <span><a href="https://github.com/zju3dv/" target="_blank">3D Vision Group</a>, <a href="http://www.zjucvg.net/" target="_blank">State Key lab of CAD&CG</a></span><br /> -->
      <!-- <span><a href="http://www.cs.zju.edu.cn/" target="_blank">College of Computer Science, Zhejiang University</a></span><br /> -->

      <a href="mailto:yeweicai@zju.edu.cn">Email</a> /
      <!-- <a onclick="_gaq.push(['_trackEvent', 'Personal', 'Download', 'CV']);" href="https://drive.google.com/open?id=1YYLEhIkchrcSeBanGsbH5snCZULFwyCJ" target="_blank">CV</a> / -->
      <!-- <a href="https://scholar.google.com/citations?hl=en&user=0LeXf0YAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Scholar</a> / -->
      <a href="https://github.com/ywcmaike" target="_blank">Github</a> /
      <a href="https://www.linkedin.com/in/weicai-ye-b9b36b129/" target="_blank">LinkedIn</a> /
      <a herf="https://scholar.google.com/citations?hl=zh-CN&user=qsMRsnsAAAAJ" target="_blank">Google Scholar</a> /
      <a href="yeweicai.pdf">CV</a>
    </div>
  </div>
</div>

<div style="clear: both;"><!-- page div -->

<div class="section">
  <h2>About</h2>
  <div class="paper">
    <p>I will be a third year Ph.D. candidate in Computer Science at <a href="http://www.zju.edu.cn/english/" target="_blank">Zhejiang University</a>, supervised by <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Prof.Guofeng Zhang</a>. 
      Prior to joining ZJU in 2018, I obtained my bachelor's degree in Software Engineering from <a href="http://en.uestc.edu.cn/" target="_blank">University of Electronic Science and Technology of China(UESTC)</a>, where I ranked 3rd of the 111 students in major and got National Encouragement Scholarship. 
    <!-- I interned at <a href="https://koolab.kujiale.com" target="_blank">KooLab</a> (summer 2019). -->
    </p>
    <br>
    <p>
    I am interested in <em>computer vision</em> and <em>machine learning</em>, particularly in 3D Vision. My current research focuses on Visual Localization, 3D Reconstruction, Scene Understanding and Video Understanding.
    </p>
  </div>
</div>

<div class="section">
  <h2>News</h2>
  <div class="paper" id="news">
    <ul>
      <li><b>[10/2020]</b> One paper is accepted to <a href="http://3dv2020.dgcv.nii.ac.jp/index.html" target="_blank">3DV 2020</a>.</li>
      <li><b>[08/2020]</b> Our team won 5th in <a href="https://www.biendata.xyz/competition/gigavision/final-leaderboard/" target="_blank">ECCV GigaVision Challenge 2020</a>.</li>
      <li><b>[07/2020]</b> One paper is accepted to <a href="http://ismar20.org/" target="_blank">ISMAR 2020</a>.</li>
      
<!--      <li><b>[08/2019]</b> We release a large photo-realistic dataset, <a href="http://structured3d-dataset.org" target="_blank">Structured3D dataset</a>, for data-driven structured 3D reconstruction!</li>-->
<!--      <li><b>[02/2019]</b> Three papers are accepted to <a href="http://cvpr2019.thecvf.com" target="_blank">CVPR 2019</a>.</li>-->
<!--      <a shape="rect" href="javascript:toggleabs('news')" class="toggleabs">Archive</a>-->
<!--      <span class="news">-->
<!--      <li><b>[06/2019]</b> Our paper on planar reconstruction is invited to be presented at the <a href="https://3dscenegen.github.io/" target="_blank">3D Scene Generation Workshop</a> at <a href="http://cvpr2019.thecvf.com" target="_blank">CVPR 2019</a>.</li>-->
<!--      <li><b>[07/2017]</b> Second place of <a href="https://www.vision.ee.ethz.ch/webvision/2017/workshop.html" target="_blank">WebVision Challenge</a> on Image Classification Task (Top University Team).</li>-->
      </span>
    </ul>
  </div>
</div>

<!-- - TEMPLATE
<div class="paper" id="X">
  <div>
    <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'X-eccv-14']);" href="papers/X.pdf">X</a><br />
    <strong>Ross Girshick</strong> <br /> European Conference on Computer Vision (ECCV), 2014<br />
    <a shape="rect" href="javascript:toggleabs('X')" class="toggleabs">abstract</a> /
    <a shape="rect" href="javascript:togglebib('X')" class="togglebib">bibtex</a>
    <pre xml:space="preserve">
    @inproceedings{X,
        Author    = {X},
        Title     = {X},
        Booktitle = {Proceedings of the European
                     Conference on Computer Vision ({ECCV})},
        Year      = {2014}}
    </pre>
    <span class="blurb">X</span>
  </div>
  <div class="spanner"></div>
</div>
-->

<div class="section">
  <h2 id="reports">Projects and Publication</h2>
  <div class="paper" id="ISMAR2020">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'ISMAR2020']);" href="https://arxiv.org/abs/1908.00222"> -->
      <a href="https://ieeexplore.ieee.org/document/9284705">
        Learning Bipartite Graph Matching for Camera Localization
      </a><br />
      Hailin Yu, <strong>Weicai Ye</strong>, Youji Feng, Hujun Bao, Guofeng Zhang<br />
      <strong>IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 2020</strong> <br />
      <span>
        2D-3D matching is an essential step for visual localization, where the
accuracy of the camera pose is mainly determined by the quality of
2D-3D correspondences. The matching is typically achieved by the
nearest neighbor search of local features. Many existing works have
shown impressive results on both efficiency and accuracy. Recently
emerged learning-based features further improve the robustness
compared to the traditional hand-crafted ones. However, it is still
not easy to establish enough correct matches in challenging scenes
with illumination changes or repetitive patterns due to the intrinsic
local property of the features. In this work, we propose a novel
method to deal with 2D-3D matching in a very robust way. We
first establish as many potential correct matches as possible using
the local similarity. Then we construct a bipartite graph and use a
convolutional neural network, referred to as Bipartite Graph Network
(BGNet), to extract the global geometric information. The network
predicts the likelihood of being an inlier for each edge and outputs
the globally optimal one-to-one correspondences with a Hungarian
pooling layer. The experiments show that the proposed method is
able to find more correct matches, and improve localization in both
robustness and accuracy. The results on multiple visual localization
datasets are obviously better than the existing state-of-the-arts, which
demonstrate the effectiveness of the proposed method.      </span> <br/>
<a href="https://docs.google.com/presentation/d/1fEzsb3RmZIa8kfDGopNcZ62agY5FuTeQJu3xj9XjNrM/mobilepresent?slide=id.ga6f4ee1874_0_1"> poster </a><br/>
      <!-- <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'ZhengZLTGZ19']);" href="https://drive.google.com/file/d/17F_jIfY_QKFNmsOSvzUFZwWKrr6YUMnQ" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Webpage', 'Download', 'ZhengZLTGZ19']);" href="https://structured3d-dataset.org" target="_blank">webpage</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'ZhengZLTGZ19']);" href="https://github.com/bertjiazheng/Structured3D" target="_blank">code</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Zhihu', 'Download', 'ZhengZLTGZ19']);" href="https://zhuanlan.zhihu.com/p/77555645" target="_blank">zhihu blog (chinese)</a> /
      <a shape="rect" href="javascript:togglebib('ZhengZLTGZ19')" class="togglebib">bibtex</a><br /> -->
   <!--    <pre xml:space="preserve">
@article{ZhengZLTGZ19,
  author    = {Jia Zheng and
               Junfei Zhang and
               Jing Li and
               Rui Tang and
               Shenghua Gao and
               Zihan Zhou},
  title     = {Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling},
  journal   = {CoRR},
  volume    = {abs/1908.00222},
  year      = {2019}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>
  <div class="paper" id="3DV2020">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'IJCAI2020']);"> -->
      <a>
        Saliency Guided Subdivision for Single-View Mesh Reconstruction
      </a><br />
      Hai Li*, <strong>Weicai Ye*</strong>, Guofeng Zhang<br />
      <!-- <a href="http://cvpr2020.thecvf.com/" target="_blank">CVPR 2020</a> / -->
<!--      MM2020 -->
      <strong>International Conference on 3D Vision (3DV 2020)</strong> <br />
      <span>
        In this paper, we present a novel deep architecture to recover a 3D shape in triangular mesh from a single image based on mesh deformation. Most existing deformation-based methods produce uniform mesh predictions by repeatedly applying global subdivision but fail to require the highlighted details due to the memory limits. To address this problem, we propose a novel saliency guided subdivision method to achieve the trade-off between detail generation and memory consumption. Instead of using local geometric cues such as curvature, we introduce a global point-based saliency voting operation to guide the adaptive mesh subdivision and deformation explicitly. 
Moreover, we propose the oriented chamfer loss to mitigate the mesh self-intersection problem in subdivision. We further make our network configurable and explore the best structure combination. Extensive experiments show that our method can both produce visually pleasing results with fine details and achieve better performance compared to other state-of-the-art methods.      </span>
<!--       <a shape="rect" href="javascript:togglebib('JinXZZTXYG20')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{JinXZZTXYG20,
  author    = {Lei Jin and
               Yanyu Xu and
               Jia Zheng and
               Junfei Zhang and
               Rui Tang and
               Shugong Xu and
               Jingyi Yu and
               Shenghua Gao},
  title     = {Geometric Structure Based and Regularized Depth Estimation From
               360 Indoor Imagery},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  year      = {2020}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>
  

  <!--------------------------------------------------------------------------->
  <!--YuZLZG19-->
  <!--------------------------------------------------------------------------->
  <div class="paper" id="2020">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'MM2020']);" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Single-Image_Piece-Wise_Planar_3D_Reconstruction_via_Associative_Embedding_CVPR_2019_paper.pdf"> -->
      <a>
        SuperPlane: 3D Plane Detection and Description from a Single Image
      </a><br />
      <strong>Weicai Ye</strong>, Guofeng Zhang<br />
      It has been summited to VR2021. <br />
      <span>
        We present a novel end-to-end plane detection and description network named SuperPlane to tackle the challenging conditions in matching problems. Our network takes a single image as input and extracts 3D planes and generates corresponding descriptors simultaneously. We also propose a mask-attention module and an instance-triplet loss to improve the distinctiveness of the plane descriptor. The detected plane descriptors are treated as discrete distributions and an Area-Aware Kullback-Leibler (KL) Divergence Retrieval method is proposed for the Image-Based Localization (IBL) task. Extensive experiments show that our method outperforms state-of-the-art methods and retains good generalization capacity. An AR application is presented to demonstrate the effectiveness of the proposed method.
      </span>
      <!-- <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'YuZLZG19']);" href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Yu_Single-Image_Piece-Wise_Planar_CVPR_2019_supplemental.pdf" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'YuZLZG19']);" href="https://github.com/svip-lab/PlanarReconstruction" target="_blank">code</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Poster', 'Download', 'YuZLZG19']);" href="https://drive.google.com/open?id=1th86ZZfoJ_6dLTcpOXfIpzUj7vfFVxMi" target="_blank">poster</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Slide', 'Download', 'YuZLZG19']);" href="https://drive.google.com/open?id=1YaEQ_PQg_H-0rDSrA1tDFSWOK8ZkGnBv" target="_blank">slide</a> /
      <a shape="rect" href="javascript:togglebib('YuZLZG19')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{YuZLZG19,
  author    = {Zehao Yu and
               Jia Zheng and
               Dongze Lian and
               Zihan Zhou and
               Shenghua Gao},
  title     = {Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  pages     = {1029--1037},
  year      = {2019}
}
</pre>
    </div>
    <div class="spanner"></div> -->
  </div>
  <!--------------------------------------------------------------------------->
  <!--LianLZLG19-->
  <!--------------------------------------------------------------------------->
  <!-- <div class="paper" id="LianLZLG19">
    <div class="wide">
      <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'LianLZLG19']);" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Lian_Density_Map_Regression_Guided_Detection_Network_for_RGB-D_Crowd_Counting_CVPR_2019_paper.pdf">
        Density Map Regression Guided Detection Network for RGB-D Crowd Counting and Localization
      </a><br />
      Dongze Lian*, Jing Li*, <strong>Jia Zheng</strong>, Weixin Luo, Shenghua Gao<br />
      <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'LianLZLG19']);" href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Lian_Density_Map_Regression_CVPR_2019_supplemental.pdf" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'LianLZLG19']);" href="https://github.com/svip-lab/RGBD-Counting" target="_blank">code</a> /
      <a shape="rect" href="javascript:togglebib('LianLZLG19')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{LianLZLG19,
  author    = {Dongze Lian and
               Jing Li and
               Jia Zheng and
               Weixin Luo and
               Shenghua Gao},
  title     = {Density Map Regression Guided Detection Network for
               RGB-D Crowd Counting and Localization},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  pages     = {1821--1830},
  year      = {2019}
}
</pre>
    </div>
    <div class="spanner"></div>
  </div> -->
  <!--------------------------------------------------------------------------->
  <!--ZhangLBZWHLXG19-->
  <!--------------------------------------------------------------------------->
 <!--  <div class="paper" id="ZhangLBZWHLXG19">
    <div class="wide">
      <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'ZhangLBZWHLXG19']);" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_PPGNet_Learning_Point-Pair_Graph_for_Line_Segment_Detection_CVPR_2019_paper.pdf">
        PPGNet: Learning Point-Pair Graph for Line Segment Detection
      </a><br />
      Ziheng Zhang*, Zhengxin Li*, Ning Bi, <strong>Jia Zheng</strong>, Jinlei Wang, Kun Huang, Weixin Luo, Yanyu Xu, Shenghua Gao<br />
      <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'ZhangLBZWHLXG19']);" href="https://github.com/svip-lab/PPGNet" target="_blank">code</a> /
      <a shape="rect" href="javascript:togglebib('ZhangLBZWHLXG19')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{ZhangLBZWHLXG19,
  author    = {Ziheng Zhang and
               Zhengxin Li and
               Ning Bi and
               Jia Zheng and
               Jinlei Wang and
               Kun Huang and
               Weixin Luo and
               Yanyu Xu and
               Shenghua Gao},
  title     = {PPGNet: Learning Point-Pair Graph for Line Segment Detection},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  pages     = {7105--7114},
  year      = {2019}
}
</pre>
    </div> -->
    <div class="spanner"></div>
  </div>

  <div class="paper" id="2021">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'ISMAR2020']);" href="https://arxiv.org/abs/1908.00222"> -->
      <a>
        Cross-Modal Shop Clothes Retrieval for Live Delivery
      </a><br />
      <strong>Weicai Ye</strong>, GuoFeng Zhang<br />
      Extra experiments have been conducted and the paper is writing now.  <br />
      <span>
        Live streaming is an important way for Taobao to connect goods with consumers. Buyers buy their favorite products while watching the live broadcast. In a single Taobao live broadcast, the anchor will often display, try, and introduce hundreds of products. If a buyer wants to purchase the product being explained, they need to list the products associated with the live broadcast to manually select, which greatly affects the user's purchasing efficiency and user experience. To tackle this problem, we propose a novel one-stage clothes retrieval framework which directly acquires the clothes' location and ID embedding. A motion capture module is proposed to effectively identify the anchor's intentions from the live broadcast and retrieve products related to the live broadcast merchandise. In order to exploit text information, we propose a cross-domain fusion method. Then a rerank post-process is followed to further improve retrieval accuracy. Extensive experiments show that our algorithm achieves the competitive results compared to the detect-match approaches, and will be applied to the industrial environment of Taobao live broadcast with goods to achieve a balance between accuracy and performance.     </span> <br/>
        <li><strong>Ranked 6th among the 1945 teams in the Taobao Live Product Identification Contest </strong> </li> <br/>
      <!-- <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'ZhengZLTGZ19']);" href="https://drive.google.com/file/d/17F_jIfY_QKFNmsOSvzUFZwWKrr6YUMnQ" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Webpage', 'Download', 'ZhengZLTGZ19']);" href="https://structured3d-dataset.org" target="_blank">webpage</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'ZhengZLTGZ19']);" href="https://github.com/bertjiazheng/Structured3D" target="_blank">code</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Zhihu', 'Download', 'ZhengZLTGZ19']);" href="https://zhuanlan.zhihu.com/p/77555645" target="_blank">zhihu blog (chinese)</a> /
      <a shape="rect" href="javascript:togglebib('ZhengZLTGZ19')" class="togglebib">bibtex</a><br /> -->
   <!--    <pre xml:space="preserve">
@article{ZhengZLTGZ19,
  author    = {Jia Zheng and
               Junfei Zhang and
               Jing Li and
               Rui Tang and
               Shenghua Gao and
               Zihan Zhou},
  title     = {Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling},
  journal   = {CoRR},
  volume    = {abs/1908.00222},
  year      = {2019}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>

<div class="paper" id="2021">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'ISMAR2020']);" href="https://arxiv.org/abs/1908.00222"> -->
      <a>
        Video Object Segmentation
      </a><br />
      <strong>Weicai Ye</strong>, GuoFeng Zhang<br />
      Extra experiments have been conducted.  <br />
      <span>
        Propose a novel architecture for video object segmentation     </span> <br/>
        <!-- <li><strong>the baseline(reimplementation of STM) Ranked 33th among the 702 teams in the MEDIA AI Alibaba Entertainment Algorithm Challenge-High-precision video character segmentation track </strong> </li> <br/> -->
      <!-- <a onclick="_gaq.push(['_trackEvent', 'Supp', 'Download', 'ZhengZLTGZ19']);" href="https://drive.google.com/file/d/17F_jIfY_QKFNmsOSvzUFZwWKrr6YUMnQ" target="_blank">supp</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Webpage', 'Download', 'ZhengZLTGZ19']);" href="https://structured3d-dataset.org" target="_blank">webpage</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Code', 'Download', 'ZhengZLTGZ19']);" href="https://github.com/bertjiazheng/Structured3D" target="_blank">code</a> /
      <a onclick="_gaq.push(['_trackEvent', 'Zhihu', 'Download', 'ZhengZLTGZ19']);" href="https://zhuanlan.zhihu.com/p/77555645" target="_blank">zhihu blog (chinese)</a> /
      <a shape="rect" href="javascript:togglebib('ZhengZLTGZ19')" class="togglebib">bibtex</a><br /> -->
   <!--    <pre xml:space="preserve">
@article{ZhengZLTGZ19,
  author    = {Jia Zheng and
               Junfei Zhang and
               Jing Li and
               Rui Tang and
               Shenghua Gao and
               Zihan Zhou},
  title     = {Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling},
  journal   = {CoRR},
  volume    = {abs/1908.00222},
  year      = {2019}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>

  <div style="padding-left: 1em">
    * indicates equal contribution
  </div>
</div>

<div class="section">
  <h2 id="reports">Experience</h2>
  <div class="paper" id="Sensetime2018">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'IJCAI2020']);"> -->
      <a>
        3D Reconstruction of Indoor Scene of RGB-D Images
      </a><br />
      <strong>SenseTime Group Ltd.</strong>  Hangzhou, China,  Jan 2018 -- May 2018 <br/>
      3D Vision-Researcher Internship <br/>
      
      <span>
        Integrate traditional RGBD SLAM and semantic segmentation, plane detection to form SemanticSLAM. <br/>
        Different modules cooperate with each other to effectively improve the quality of localization and mapping. <br/>
        The system can perform real-time reconstruction of room-scale indoor scenes.
      </span>
<!--       <a shape="rect" href="javascript:togglebib('JinXZZTXYG20')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{JinXZZTXYG20,
  author    = {Lei Jin and
               Yanyu Xu and
               Jia Zheng and
               Junfei Zhang and
               Rui Tang and
               Shugong Xu and
               Jingyi Yu and
               Shenghua Gao},
  title     = {Geometric Structure Based and Regularized Depth Estimation From
               360 Indoor Imagery},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  year      = {2020}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>
  <div class="paper" id="Baidu2017">
    <div class="wide">
      <!-- <a class="paper" onclick="_gaq.push(['_trackEvent', 'Pub', 'Download', 'IJCAI2020']);"> -->
      <a>
        Video Search System
      </a><br />
      <strong> Baidu Inc. </strong> Beijing, China, Feb 2017 -- Jul 2017<br/>
      Software Engineer Internship <br />
      <span>
        Cooperate with colleagues to develop millisecond-level response video search services that can support hundreds of millions of highly concurrent retrieval needs. <br/>
        Cooperate with colleagues to develop rearrangement strategies such as video resolution, cross-modal fusion, etc to improve the quality of video retrieval results.<br/>
        The system can retrieve the related video with the given text in real time and the service is online.
      </span>
<!--       <a shape="rect" href="javascript:togglebib('JinXZZTXYG20')" class="togglebib">bibtex</a><br />
      <pre xml:space="preserve">
@inproceedings{JinXZZTXYG20,
  author    = {Lei Jin and
               Yanyu Xu and
               Jia Zheng and
               Junfei Zhang and
               Rui Tang and
               Shugong Xu and
               Jingyi Yu and
               Shenghua Gao},
  title     = {Geometric Structure Based and Regularized Depth Estimation From
               360 Indoor Imagery},
  booktitle = {Proceedings of The IEEE Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  year      = {2020}
}
</pre> -->
    </div>
    <div class="spanner"></div>
  </div>
</div>

<div class="section">
  <h2>Awards & Honors</h2>
  <div class="paper">
    <ul>
      <!-- <li>Ranked 33th among the 702 teams in the MEDIA AI Alibaba Entertainment Algorithm Challenge-High-precision video character segmentation track --2020.07 </li> -->
      <li><strong>5th </strong> in ECCV GigaVision Challenge 2020 </li>
      <li><strong>6th </strong> among the 1945 teams in the Taobao Live Product Identification Contest --2020 </li>
      <li>Zhijun He Outstanding Scholarship                                       -- 2019 </li>
      <li>Chiang Chen Industrial Charity Foundation Grant                         -- 2019 </li>
      <li>Excellence Price of the 1st IKCEST "The Belt and Road" International Big Data Competition. --2019</li>
      <li><strong>Chanpion</strong> of 2018 Cloudwalk Headcount Challenge with 31,500¥ Bonus       -- 2019 </li>
      <li>Graduate Student Scholarship                                            -- 2018,2019,2020 </li>
      <li>National Encouragement Scholarship（Ranked 3rd of 111 Students）           -- 2017 </li>
      <li><strong>Meritorious Winner </strong> in Mathematical Contest Modeling                     -- 2017 </li>
      <li>First Prize in Sichuan Province Contest District in China Undergraduate Mathematical Contest in Modeling -- 2016 </li>
    </ul>
  </div>
</div>

<div class="section">
  <h2>Teaching</h2>
  <div class="paper">
    <ul>
      <li>Introduction to Python Programming, Teaching Assistant (TA)     - Spring 2020</li>
      <li>Introduction to Java Programming, Teaching Assistant (TA)        - Spring 2019</li>
    </ul>
  </div>
</div>

</div><!-- close page div -->

<div style="clear:both;">
  <div style="padding-left: 1em; float: left; display: block;">
    Last updated: May. 17, 2020
  </div>
  <div style="padding-right: 1em; float: right; display: block;">
    Template by <a href="http://www.rossgirshick.info/">rbg</a>.
  </div>
  <br>
</div>

<script xml:space="preserve" language="JavaScript">
  hideallbibs();
  hideallabs();
</script>

</body>

</html>
